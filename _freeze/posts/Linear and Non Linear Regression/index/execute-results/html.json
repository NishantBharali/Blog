{
  "hash": "a87fb522a6e081e977f10092b7667056",
  "result": {
    "markdown": "---\ntitle: Advanced analysis on Linear and Non-Linear Regression models\ndescription: \"A Comprehensive Analysis on Housing Market. Utilizing regression methodologies for the purpose of analyzing trends within the housing market\"\ndate: 11-24-2023\ncategories: [Machine Learning]\nimage: image2.jpg\n---\n\n## A Comprehensive Analysis on Housing Market\n\n## Utilizing linear and non-linear regression methodologies for the purpose of analyzing trends within the housing market\n\n**Introduction**\n\nIn machine learning, linear and nonlinear regression are fundamental techniques used to model relationships between variables, such as predicting housing prices based on various features in a dataset.\n\nLinear Regression is a straightforward method that assumes a linear relationship between the input features (e.g., square footage, number of bedrooms) and the target variable (housing price). It aims to find the best-fit line that minimizes the difference between predicted and actual values. Linear regression is interpretable and works well when the relationship is approximately linear.\n\nNonlinear Regression, on the other hand, allows for more complex relationships. It can capture curves, bends, and nonlinear patterns in the data. This is particularly useful when housing prices may depend on interactions between features or exhibit nonlinear behavior.\n\nThe housing dataset you are using provides a rich source of information to apply both linear and nonlinear regression techniques. By utilizing these methods, you can build predictive models that estimate housing prices accurately, taking into account the specific relationships between features and target variables, whether they are linear or nonlinear in nature. These models can guide real estate decisions, investment strategies, and market analyses more effectively, ultimately benefiting both buyers and sellers in the housing market.\n\nWe will start by loading the dataset and performing basic preprocessing, including encoding categorical variables and feature scaling.\n\n **Exploring the Dataset and Data Preprocessing**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\n\n\n# Load the dataset\nhousing_df = pd.read_csv('modified_housing_data.csv')\n\n# Handling missing values if any\nimputer = SimpleImputer(strategy='mean')\nhousing_df[['Size', 'GardenArea']] = imputer.fit_transform(housing_df[['Size', 'GardenArea']])\n\n# One-hot encoding and Scaling\ncategorical_features = ['Neighborhood']\nnumerical_features = ['Size', 'Bedrooms', 'Bathrooms', 'Age', 'GarageSize', 'GardenArea']\n\n# Create transformers\none_hot = OneHotEncoder()\nscaler = StandardScaler()\n\n# Column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', scaler, numerical_features),\n        ('cat', one_hot, categorical_features)\n    ])\n\n# Preprocessing pipeline\nprep_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n# Splitting the data\nX = housing_df.drop(['HouseID', 'Price'], axis=1)\ny = housing_df['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_prep = prep_pipeline.fit_transform(X_train)\nX_test_prep = prep_pipeline.transform(X_test)\n```\n:::\n\n\n**Visualizing Market Trend**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#Visualizing the distributions and relationships\nsns.pairplot(housing_df)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1887 height=1887}\n:::\n:::\n\n\n## Advanced Linear Regression Analysis\n\nFor the linear regression model, we'll include feature importance analysis and cross-validation.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Linear Regression Model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_prep, y_train)\n\n# Cross-Validation\ncv_scores = cross_val_score(linear_model, X_train_prep, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"CV MSE for Linear Regression:\", -np.mean(cv_scores))\n\n# Predictions and Evaluation\ny_pred_linear = linear_model.predict(X_test_prep)\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression Test MSE:\", mse_linear)\nprint(\"Linear Regression Test R2:\", r2_linear)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCV MSE for Linear Regression: 2618960221.1247973\nLinear Regression Test MSE: 2520672652.9396286\nLinear Regression Test R2: 0.9111757572070075\n```\n:::\n:::\n\n\n## Advanced Non-Linear Regression Analysis\n\nWe'll apply a more complex non-linear model, such as a Random Forest Regressor, and perform hyperparameter tuning using GridSearchCV.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Non-Linear Model - Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=42)\n\n# Hyperparameter Grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_features': ['sqrt', 'log2', None],  # Removed 'auto' and added None\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid Search\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(X_train_prep, y_train)\n\n# Best Model\nbest_rf_model = grid_search.best_estimator_\n\n# Predictions and Evaluation\ny_pred_rf = best_rf_model.predict(X_test_prep)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\nprint(\"Random Forest Test MSE:\", mse_rf)\nprint(\"Random Forest Test R2:\", r2_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 3 folds for each of 324 candidates, totalling 972 fits\nRandom Forest Test MSE: 3815896212.7555285\nRandom Forest Test R2: 0.865534268688408\n```\n:::\n:::\n\n\n## Advanced Regression Analysis\n\n**1. Exploring Feature Interactions**\n\nFeature interactions can reveal complex relationships that might not be captured by individual features alone.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Adding interaction terms\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train_poly = poly.fit_transform(X_train_prep)\nX_test_poly = poly.transform(X_test_prep)\n\n# Re-training Linear Regression with Interaction Terms\nlinear_model_interact = LinearRegression()\nlinear_model_interact.fit(X_train_poly, y_train)\n\n# Evaluating the model with interaction terms\ny_pred_interact = linear_model_interact.predict(X_test_poly)\nmse_interact = mean_squared_error(y_test, y_pred_interact)\nprint(\"MSE with Interaction Terms:\", mse_interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE with Interaction Terms: 2983950454.9349637\n```\n:::\n:::\n\n\n**2. Model Diagnostics for Linear Regression**\n\nChecking assumptions and diagnostics of linear regression to ensure the validity of the model.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Model Diagnostics\nX_train_sm = sm.add_constant(X_train_prep)  # Adding a constant\nmodel = sm.OLS(y_train, X_train_sm).fit()\nprint(model.summary())\n\n# Residuals plot\nplt.scatter(model.predict(X_train_sm), model.resid)\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Predicted')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.917\nModel:                            OLS   Adj. R-squared:                  0.914\nMethod:                 Least Squares   F-statistic:                     283.3\nDate:                Sun, 10 Dec 2023   Prob (F-statistic):          3.91e-119\nTime:                        20:22:05   Log-Likelihood:                -2932.4\nNo. Observations:                 240   AIC:                             5885.\nDf Residuals:                     230   BIC:                             5920.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.798e+05   2602.372    184.377      0.000    4.75e+05    4.85e+05\nx1           1.56e+05   3262.250     47.819      0.000     1.5e+05    1.62e+05\nx2          5.035e+04   3267.184     15.410      0.000    4.39e+04    5.68e+04\nx3          2.194e+04   3304.761      6.640      0.000    1.54e+04    2.85e+04\nx4         -1.011e+04   3260.647     -3.100      0.002   -1.65e+04   -3684.133\nx5         -2624.2257   3281.530     -0.800      0.425   -9089.930    3841.478\nx6           -41.7240   3263.210     -0.013      0.990   -6471.330    6387.882\nx7          1.215e+05   5467.126     22.218      0.000    1.11e+05    1.32e+05\nx8           1.26e+05   5609.374     22.456      0.000    1.15e+05    1.37e+05\nx9          1.184e+05   6164.082     19.216      0.000    1.06e+05    1.31e+05\nx10         1.139e+05   5567.918     20.463      0.000    1.03e+05    1.25e+05\n==============================================================================\nOmnibus:                        0.912   Durbin-Watson:                   2.215\nProb(Omnibus):                  0.634   Jarque-Bera (JB):                0.697\nSkew:                          -0.123   Prob(JB):                        0.706\nKurtosis:                       3.097   Cond. No.                     6.20e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 7.83e-30. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=629 height=449}\n:::\n:::\n\n\n**3. Learning Curves**\n\nUnderstanding how model performance changes as the training set size increases.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Learning curve function\ndef plot_learning_curve(estimator, X, y, title):\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    plt.grid()\n    plt.show()\n\n# Plotting learning curve for Linear Regression\nplot_learning_curve(linear_model, X_train_prep, y_train, \"Linear Regression Learning Curve\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=617 height=449}\n:::\n:::\n\n\n**4. Ensemble Methods**\n\nCombining multiple regression models to improve predictive performance.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Gradient Boosting Regressor\ngb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_gb = gb_reg.predict(X_test_prep)\nmse_gb = mean_squared_error(y_test, y_pred_gb)\nprint(\"Gradient Boosting Regressor MSE:\", mse_gb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradient Boosting Regressor MSE: 3673304675.984175\n```\n:::\n:::\n\n\n## Deeper Regression Analysis\n\n**Feature Importance-Based Selection**\n\nFirst, let's use a model to identify the most important features and then retrain our models using only these features.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Feature Importance with Random Forest\nrf_for_importance = RandomForestRegressor()\nrf_for_importance.fit(X_train_prep, y_train)\n\n# Get feature importances and corresponding feature names\nimportances = rf_for_importance.feature_importances_\nfeature_names = prep_pipeline.get_feature_names_out()\n\n# Creating a DataFrame for visualization\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n\n# Adjusting the threshold for feature selection\ntop_features = importance_df[importance_df['Importance'].cumsum() <= 0.90]['Feature']\n\n# Ensure that top_features is not empty\nif len(top_features) == 0:\n    raise ValueError(\"No features selected. Consider loosening the feature selection criterion.\")\n\nX_train_top = X_train_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\nX_test_top = X_test_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\n\n# Re-train models with top features\nlinear_model_top = LinearRegression()\nlinear_model_top.fit(X_train_top, y_train)\n\n# Evaluation\ny_pred_top = linear_model_top.predict(X_test_top)\nmse_top = mean_squared_error(y_test, y_pred_top)\nprint(\"Top Features Linear Regression MSE:\", mse_top)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop Features Linear Regression MSE: 5967275811.385474\n```\n:::\n:::\n\n\n## Advanced Non-Linear Models\n\nIncorporating more complex non-linear models such as Support Vector Regression and Neural Networks.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Support Vector Regression\nsvr = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_svr = svr.predict(X_test_prep)\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nprint(\"Support Vector Regression MSE:\", mse_svr)\n\nnn_reg = MLPRegressor(hidden_layer_sizes=(100, 50), \n                      max_iter=5000, \n                      learning_rate_init=0.001, \n                      solver='adam', \n                      early_stopping=True, \n                      n_iter_no_change=10,\n                      random_state=42)\nnn_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_nn = nn_reg.predict(X_test_prep)\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nprint(\"Neural Network Regression MSE:\", mse_nn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Regression MSE: 28406556463.671677\nNeural Network Regression MSE: 397201687873.44415\n```\n:::\n:::\n\n\n**Building and Evaluating the Linear Regression Model**\n\n**A. Final Model Training**\n\nWe'll train the final models using the best parameters found from previous steps.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Linear Regression\nlinear_model_final = LinearRegression()\nlinear_model_final.fit(X_train_prep, y_train)\n\n# Neural Network\nnn_model_final = MLPRegressor(hidden_layer_sizes=(100, 50), \n                              max_iter=5000, \n                              learning_rate_init=0.001, \n                              solver='adam', \n                              early_stopping=True, \n                              n_iter_no_change=10,\n                              random_state=42)\nnn_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_linear = linear_model_final.predict(X_test_prep)\ny_pred_nn = nn_model_final.predict(X_test_prep)\n```\n:::\n\n\n**B. Evaluation Metrics**\n\nCalculating and printing evaluation metrics for both models.\nWe'll train the final models using the best parameters found from previous steps.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Evaluation for Linear Regression\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression - MSE:\", mse_linear, \"R2:\", r2_linear)\n\n# Evaluation for Neural Network\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\nprint(\"Neural Network - MSE:\", mse_nn, \"R2:\", r2_nn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression - MSE: 2520672652.9396286 R2: 0.9111757572070075\nNeural Network - MSE: 397201687873.44415 R2: -12.996715964015438\n```\n:::\n:::\n\n\n**C. Plotting Model Performance**\n\nVisualizing the performance of the models using scatter plots and residual plots.\n\n**I. Scatter Plot for Predictions.**\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 6))\n\n# Scatter plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Scatter plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=1142 height=566}\n:::\n:::\n\n\n**II. Residual Plot**\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 6))\n\n# Residual plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Residual plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=1142 height=566}\n:::\n:::\n\n\n**Building and Evaluating Non-Linear Model (Support Vector Regression)**\n\n**A. Training the Support Vector Regressor**\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.svm import SVR\n\n# Support Vector Regression\nsvr_model_final = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_svr = svr_model_final.predict(X_test_prep)\n```\n:::\n\n\n**B. Evaluation Metrics for SVR**\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Evaluation for Support Vector Regression\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\nprint(\"Support Vector Regression - MSE:\", mse_svr, \"R2:\", r2_svr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Regression - MSE: 28406556463.671677 R2: -0.0009990251211158263\n```\n:::\n:::\n\n\n**C. Plotting Comparisons**\n\nVisualizing the performance of the Linear Regression, Neural Network, and Support Vector Regression models.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplt.figure(figsize=(18, 6))\n\n# Linear Regression\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Neural Network\nplt.subplot(1, 3, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\n# Support Vector Regression\nplt.subplot(1, 3, 3)\nplt.scatter(y_test, y_pred_svr, alpha=0.5, color='green')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Support Vector Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=1718 height=566}\n:::\n:::\n\n\n**D. Residual Plot for All Models**\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nplt.figure(figsize=(18, 6))\n\n# Linear Regression Residuals\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Neural Network Residuals\nplt.subplot(1, 3, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\n# SVR Residuals\nplt.subplot(1, 3, 3)\nplt.scatter(y_pred_svr, y_test - y_pred_svr, alpha=0.5, color='green')\nplt.hlines(y=0, xmin=y_pred_svr.min(), xmax=y_pred_svr.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Support Vector Regression Residuals')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=1718 height=566}\n:::\n:::\n\n\n## Conclusion\n\nIn conclusion, the fields of random processes and probability theory, as well as linear and nonlinear regression, are vital components of machine learning when applied to diverse datasets such as weather and housing data. These foundational concepts empower us to model and make predictions in the face of inherent uncertainty, allowing for more accurate forecasts, informed decision-making, and improved insights.\n\nFor weather data analysis, random processes and probability theory enable us to understand and quantify the stochastic nature of weather patterns. Leveraging machine learning techniques on this data helps us provide accurate forecasts and anticipate extreme events, benefiting numerous sectors that rely on weather information.\n\nIn the case of housing data analysis, linear and nonlinear regression techniques enable us to model complex relationships between housing features and prices. Whether it's linear relationships for straightforward cases or nonlinear models to capture intricate patterns, these tools empower us to make more informed decisions in real estate, investments, and market analysis.\n\nIn both domains, machine learning applied to these fundamental concepts provides us with the means to extract valuable insights and make data-driven decisions, ultimately enhancing our understanding and predictive capabilities, and offering practical solutions that can improve the quality of life and the efficiency of various industries.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}