{
  "hash": "99a1e48981222e8909add5a7f9afebe6",
  "result": {
    "markdown": "---\ntitle: Advanced Clustering and Prediction techniques\ndescription: \"Employing sophisticated clustering techniques with retail customer data and comparing further with prediction models\"\ndate: 11-24-2023\ncategories: [Machine Learning]\nimage: E3K6.gif\ntoc-location: right\n---\n\n#### Advanced Clustering and Prediction techniques Using Retail Customer Data\n\n## Employing sophisticated clustering techniques with retail customer data and comparing further with prediction models\n\nClustering is a fundamental technique in machine learning that involves grouping data points so that the objects in the same group (or cluster) are more similar to each other than to those in other groups. It's a form of unsupervised learning, as the groups are not predefined but rather determined by the algorithm itself. This approach is particularly useful in understanding the structure within data, identifying patterns, and making strategic decisions.\n\nIn this blog, we will explore how to apply clustering techniques to a customer dataset. Our dataset contains customer information with attributes like Customer ID, Age, Annual Income, and Spending Score. The goal is to segment customers into distinct groups based on these features, which can help in tailoring marketing strategies, understanding customer behavior, and improving customer service.\n\n**Introduction**\n\n**Data Loading and Basic Visualization**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncustomer_df = pd.read_csv('clusteringCustomerData.csv')\nprint(customer_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   CustomerID  Age  Annual_Income  Spending_Score\n0           1   62             75              81\n1           2   65             76              23\n2           3   18             98              80\n3           4   21             48              90\n4           5   21             47               9\n```\n:::\n:::\n\n\n**Exploring Data Analysis**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#Visualize the distributions and relationships between features.\nsns.pairplot(customer_df.drop('CustomerID', axis=1))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=711 height=711}\n:::\n:::\n\n\n**Data Preprocessing**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#Encoding categorical data and scaling features.\n#Standardize the data.\n# Selecting features to be scaled\nfeatures = customer_df[['Age', 'Annual_Income', 'Spending_Score']]\n\n# Standardizing the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\n# Converting scaled features back to a DataFrame\nscaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n\n# Displaying the first few rows of the scaled data\nprint(scaled_features_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Age  Annual_Income  Spending_Score\n0  1.237684       0.746278        0.901945\n1  1.428146       0.788416       -1.020966\n2 -1.555756       1.715469        0.868791\n3 -1.365294      -0.391469        1.200327\n4 -1.365294      -0.433608       -1.485117\n```\n:::\n:::\n\n\n**Exploratory Data Analysis (EDA)**\n\nBeyond basic visualizations, we'll use EDA to understand the data distributions and potential relationships.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Visualizing the distribution of features\nplt.figure(figsize=(12, 4))\nfor i, col in enumerate(['Age', 'Annual_Income', 'Spending_Score']):\n    plt.subplot(1, 3, i+1)\n    sns.histplot(customer_df[col], kde=True)\n    plt.title(f'Distribution of {col}')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1141 height=374}\n:::\n:::\n\n\n## Advanced Clustering with Multiple Techniques\n\nWe'll explore different clustering algorithms beyond K-Means, such as Hierarchical Clustering and DBSCAN, to understand how they segment the data differently.\n\n## 1. K-Means Clustering\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#Using the Elbow Method to determine the optimal number of clusters.\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n    kmeans.fit(scaled_features)\n    wcss.append(kmeans.inertia_)\n    customer_df['KMeans_Cluster'] = kmeans.labels_\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=593 height=449}\n:::\n:::\n\n\n**Application of K-Means**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#Applying K-Means and visualizing the clusters.\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=42)\ncustomer_df['Cluster'] = kmeans.fit_predict(scaled_features)\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='Cluster', data=customer_df, palette='viridis')\nplt.title('Customer Segments')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=593 height=449}\n:::\n:::\n\n\n**K-Means Visualization**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='KMeans_Cluster', palette='viridis')\nplt.suptitle('K-Means Clustering', y=1.02)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=834 height=740}\n:::\n:::\n\n\n## 2. DBSCAN clustering\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5).fit(scaled_features_df)\ncustomer_df['DBSCAN_Cluster'] = dbscan.labels_\n```\n:::\n\n\n**Density-Based Spatial Clustering of Applications with Noise (DBSCAN) Visualization**\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Visualizing DBSCAN Clusters\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='DBSCAN_Cluster', data=customer_df, palette='viridis')\nplt.title('DBSCAN Clustering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=593 height=449}\n:::\n:::\n\n\n## 3. Hierarchical Cluster Visualization\n\nVisualize the clusters formed by each algorithm in multiple dimensions to gain deeper insights.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering  # Import AgglomerativeClustering\n\n# Performing Hierarchical Clustering\nlinked = linkage(scaled_features_df, method='ward')\n\n# Plotting the Dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n# Perform hierarchical clustering and add the cluster labels to customer_df\nclustering = AgglomerativeClustering(n_clusters=3)  # Specify the number of clusters\ncustomer_df['Hierarchical_Cluster'] = clustering.fit_predict(scaled_features_df)\n\n# Hierarchical Clustering Visualization\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='Hierarchical_Cluster', palette='viridis')\nplt.suptitle('Hierarchical Clustering', y=1.02)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=804 height=579}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=862 height=740}\n:::\n:::\n\n\n**Hierarchical Clustering Cut**\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import fcluster\n\n# Cutting the Dendrogram to form clusters\ncustomer_df['Hierarchical_Cluster'] = fcluster(linked, t=3, criterion='maxclust')\n\nsns.scatterplot(data=customer_df, x='Annual_Income', y='Spending_Score', hue='Hierarchical_Cluster', palette='viridis')\nplt.title('Hierarchical Clustering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=593 height=449}\n:::\n:::\n\n\n## Silhouette Analysis for K-means and Hierarchical Clustering\n\n**1. K-Means Clustering**\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\n\n# Calculate silhouette scores for different cluster numbers\nsilhouette_scores = []\nfor n_clusters in range(2, 11):\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    cluster_labels = kmeans.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plot silhouette scores\nplt.plot(range(2, 11), silhouette_scores, marker='o')\nplt.title('Silhouette Analysis for K-Means')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=597 height=449}\n:::\n:::\n\n\n**2. Hierarchical Clustering**\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Calculate silhouette scores for different cluster numbers in hierarchical clustering\nsilhouette_scores_hierarchical = []\nfor n_clusters in range(2, 11):\n    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n    cluster_labels = hierarchical.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores_hierarchical.append(silhouette_avg)\n\n# Plot silhouette scores for hierarchical clustering\nplt.plot(range(2, 11), silhouette_scores_hierarchical, marker='o')\nplt.title('Silhouette Analysis for Hierarchical Clustering')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=606 height=449}\n:::\n:::\n\n\n## Conclusion\n\n#In this in-depth analysis, we have explored different clustering techniques and visualized their results to segment customers in a comprehensive manner. Each method offers unique insights: K-Means provides clear segmentations, Hierarchical Clustering helps us understand the data structure, and DBSCAN identifies core and outlier points effectively. \n\nBy comparing these methods, we can choose the one that best suits our specific needs for customer segmentation. This advanced clustering analysis can guide strategic decisions, improve customer engagement, and enhance targeting in marketing campaigns.\n\n\n\n```{=html}\n<script>\nconst tooltipTriggerList = document.querySelectorAll('[data-bs-toggle=\"tooltip\"]')\nconst tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))\n</script>\n<style>\ndiv#quarto-sidebar-glass { display: none !important; }\nul.navbar-nav.navbar-nav-scroll { -webkit-flex-direction: row !important; }\n/* #quarto-sidebar { padding: 5px; }\n#quarto-sidebar > * { padding: 5px; }\ndiv.sidebar-menu-container > * { padding: 5px 5px 5px 5px; }\n#quarto-margin-sidebar { padding: 40px; } */\n</style>\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}