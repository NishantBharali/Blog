{
  "hash": "98e09d668f7a3af80ac647462af304bc",
  "result": {
    "markdown": "---\ntitle:  Anomaly/Outlier Detection in a Financial Dataset\ndescription: \"Detecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention\"\ndate: 11-24-2023\ncategories: [machine learning]\nimage: image2.jpg\n---\n\n## Detecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention\n\n## Introduction\n\nIn the modern world of finance, data plays a pivotal role in decision-making processes. Financial datasets often contain critical information about customers, transactions, and more. One crucial aspect of financial data analysis is the detection of anomalies or outliers. Anomalies are data points that significantly deviate from the norm or expected behavior. Detecting these anomalies is essential for fraud detection, risk management, and maintaining data integrity.\n\nThis report explores the application of anomaly/outlier detection techniques to a financial dataset. The dataset under consideration contains information such as Age, Income, Transaction Amount, Transaction Type, and Location. We aim to identify unusual patterns or data points within this dataset that may require further investigation.\n\n\n**Advanced Analysis**\n\nI will now conduct an advanced analysis of this dataset, focusing on the following aspects:\n\nData Exploration: Understanding the basic statistics of the dataset.\nVisualization: Plotting the data to observe any unusual patterns.\nAnomaly Detection Techniques: Implementing various methods to detect outliers, such as statistical methods, clustering-based methods, and machine learning models.\nFor this analysis, I will be using Python libraries like Pandas, NumPy, Matplotlib, Seaborn, and Scikit-Learn. Let's start with the data exploration and visualization.\n\n**Analysis Overview**\n\n**Data Exploration**\n\nThe descriptive statistics of the dataset provide a basic understanding of its features:\n\nA. Age: Ranges from 18 to 69, with an average of around 43 years.\nB. Income: The average income is approximately $51,518, with a wide range from around $988 to $181,196, suggesting significant variance.\nC. Transaction Amount: On average, transactions are around $200, but there are values as high as $1,999, which might indicate potential outliers.\n\n**Visualization Insights**\n\nA. Histograms: These show the distribution of numerical features. While age appears fairly uniformly distributed, income and transaction amounts show right-skewed distributions.\nB. Boxplots: The boxplots highlight potential outliers, especially in the income and transaction amount data.\nPCA Scatter Plot: After applying PCA for dimensionality reduction, we get a 2D visualization of the scaled numerical data. This plot can help us identify clusters and potential outliers visually.\n\n**Next Steps for Anomaly Detection**\n\nBased on these visualizations and statistics, the next step is to apply anomaly detection techniques. I will use the Isolation Forest algorithm, a popular method for outlier detection, especially effective with high-dimensional datasets. This method isolates anomalies instead of profiling normal data points. Anomalies are few and different, hence easier to isolate.\n\n**Data Exploration and Analysis**\n\nI applied the Isolation Forest algorithm to our scaled numerical data and identify the anomalies. Let's proceed with this analysis.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Loading the dataset\ndf = pd.read_csv('anomaly_detection_dataset.csv')\n\n# Data Exploration: Descriptive statistics\ndescriptive_stats = df.describe()\n# Display basic statistics\n\n# Visualization\n# Histograms for numerical data\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndf[['Age', 'Income', 'Transaction Amount']].hist(bins=15, ax=axes, color='skyblue')\nplt.suptitle('Histograms of Numerical Features')\n\n# Boxplot for numerical data to check for outliers\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\nsns.boxplot(data=df[['Age', 'Income', 'Transaction Amount']], ax=axes[0])\nsns.boxplot(data=df[['Income']], ax=axes[1])\nsns.boxplot(data=df[['Transaction Amount']], ax=axes[2])\nplt.suptitle('Boxplots of Numerical Features')\n\n# Preparing data for anomaly detection\n# Standardizing the numerical data\nscaler = StandardScaler()\nscaled_numerical_data = scaler.fit_transform(df[['Age', 'Income', 'Transaction Amount']])\n\n# Applying PCA for dimensionality reduction (2D visualization)\npca = PCA(n_components=2)\npca_results = pca.fit_transform(scaled_numerical_data)\npca_df = pd.DataFrame(data=pca_results, columns=['PC1', 'PC2'])\n\n# Scatter plot of PCA results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', data=pca_df)\nplt.title('PCA of Scaled Numerical Data')\n\nplt.show()\n\ndescriptive_stats, pca_df.head()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=1385 height=459}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=1418 height=459}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=810 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(               Age         Income  Transaction Amount\n count  1000.000000    1000.000000         1000.000000\n mean     43.267000   51518.424999          200.855857\n std      15.242311   18506.474035          197.923861\n min      18.000000     988.660890            0.381117\n 25%      30.000000   39745.904804           79.905356\n 50%      43.000000   50483.467494          162.361081\n 75%      56.000000   60698.045016          264.145550\n max      69.000000  181196.443031         1999.137390,\n         PC1       PC2\n 0 -0.301194 -1.080589\n 1 -0.282737 -0.824039\n 2 -0.474600  2.067434\n 3  0.344241  0.305833\n 4 -0.009123  0.894454)\n```\n:::\n:::\n\n\n**Data Visualization**\n\nNext, we visualize the data to identify any obvious outliers or patterns.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histograms for numerical features\ndf.hist(bins=15, figsize=(15, 6))\nplt.show()\n\n# Box plots for numerical features to identify outliers\ndf.plot(kind='box', subplots=True, layout=(2,3), figsize=(15, 8))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1170 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=1161 height=310}\n:::\n:::\n\n\n**Analysis Results: Anomaly Detection with Isolation Forest**\n\n*Anomaly Detection*\n\nThe Isolation Forest algorithm was applied to the scaled numerical data, and it identified 50 anomalies in our dataset. This is consistent with the initial contamination rate set to 5% of the data.\n\n*Visualization of Detected Anomalies*\n\nThe scatter plot based on the PCA results with anomalies highlighted shows:\n\nA. Normal data points in blue.\nB. Anomalies marked in red.\n\nThese anomalies represent unusual patterns in terms of age, income, and transaction amounts, as identified by the Isolation Forest algorithm.\n\n*Interpretation*\n\nThe visualization clearly shows that the anomalies are distinct from the bulk of the data, signifying their outlier status. These could represent unusual financial transactions or demographic anomalies that would be of interest in real-world scenarios like fraud detection or targeted marketing.\n\n\n**Anomaly Detection Using Isolation Forest**\n\nWe apply the Isolation Forest algorithm to detect anomalies in the dataset. This method is effective for high-dimensional datasets and does not require prior knowledge of the number of anomalies.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))\n\n# Applying Isolation Forest\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=0)\npredictions = iso_forest.fit_predict(scaled_data)\n\n# Add a column for anomaly (1 for normal, -1 for anomaly)\ndf['anomaly'] = predictions\n\n# Count the number of anomalies\nanomaly_count = df['anomaly'].value_counts()\nprint(anomaly_count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nanomaly\n 1    835\n-1    165\nName: count, dtype: int64\n```\n:::\n:::\n\n\n**Visualizing the Anomalies**\n\nWe can visualize the anomalies in the context of two principal components.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n# PCA for dimensionality reduction for visualization\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_data)\n\ndf['pca1'] = pca_result[:, 0]\ndf['pca2'] = pca_result[:, 1]\n\n# Scatter plot of the PCA results colored by anomaly\nsns.scatterplot(x='pca1', y='pca2', hue='anomaly', data=df, palette={1: 'blue', -1: 'red'})\nplt.title('Anomalies in the PCA Plane')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=587 height=449}\n:::\n:::\n\n\n**K-Means Clustering for Anomaly Detection**\n\nK-Means can be used for anomaly detection by identifying small clusters as anomalies.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# K-Means Clustering with explicit n_init parameter\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=0).fit(scaled_data)\ndf['cluster'] = kmeans.labels_\n\n# Detecting outliers as the points farthest from the centroids\ndistances = kmeans.transform(scaled_data)\ndf['distance_to_centroid'] = np.min(distances, axis=1)\noutlier_threshold = np.percentile(df['distance_to_centroid'], 95)\ndf['outlier'] = df['distance_to_centroid'] > outlier_threshold\n```\n:::\n\n\n**K-Means Clustering Visualization**\n\nAfter running the K-Means algorithm, we can plot the data points and color them by their cluster. Points classified as outliers will be highlighted.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette='Set1')\nsns.scatterplot(x=df[df['outlier']]['pca1'], y=df[df['outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('K-Means Clustering')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=810 height=523}\n:::\n:::\n\n\n**DBSCAN for Anomaly Detection**\n\nDBSCAN is effective in identifying regions of high density and isolating outliers as points in low-density areas.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN Clustering\ndbscan = DBSCAN(eps=0.5, min_samples=10).fit(scaled_data)\ndf['dbscan_labels'] = dbscan.labels_\n\n# Anomalies are points with label -1\ndf['dbscan_outlier'] = df['dbscan_labels'] == -1\n```\n:::\n\n\n**DBSCAN Visualization**\n\nFor DBSCAN, we will plot the data points and color them based on their cluster, highlighting the anomalies.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# DBSCAN Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='dbscan_labels', data=df, palette='Set2')\nsns.scatterplot(x=df[df['dbscan_outlier']]['pca1'], y=df[df['dbscan_outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('DBSCAN Clustering')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=810 height=523}\n:::\n:::\n\n\n**Local Outlier Factor (LOF)**\n\nLOF computes a score reflecting the degree of abnormality of the data, considering local density.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# LOF for anomaly detection\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\nlof_labels = lof.fit_predict(scaled_data)\n\ndf['lof_outlier'] = lof_labels == -1\n```\n:::\n\n\n**LOF Visualization**\n\nLocal Outlier Factor results can be visualized by highlighting the points that are identified as outliers.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# LOF Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='lof_outlier', data=df)\nplt.title('Local Outlier Factor')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=810 height=523}\n:::\n:::\n\n\n**T-SNE for Visualization of High-Dimensional Data**\n\nT-SNE is effective for visualizing high-dimensional data and its anomalies in a lower-dimensional space.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make sure to replace 'scaled_data' with your actual scaled data variable name\ntsne = TSNE(n_components=2, random_state=0)\ntsne_results = tsne.fit_transform(scaled_data)\n\n# Adding T-SNE results to your DataFrame\ndf['tsne1'] = tsne_results[:, 0]\ndf['tsne2'] = tsne_results[:, 1]\n\n# Use 'dbscan_labels' or another column of your choice for coloring the points\ncolumn_for_coloring = 'dbscan_labels'  # Replace with your chosen column\n\n# T-SNE Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='tsne1', y='tsne2', hue=column_for_coloring, data=df, palette='Set2', legend=\"full\")\nplt.title('T-SNE Visualization')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=819 height=523}\n:::\n:::\n\n\nT-SNE results can be plotted to show how the data is distributed in the reduced-dimensional space, highlighting the anomalies detected.\n\nThese techniques represent a deeper dive into anomaly detection, leveraging different approaches from clustering, neural networks, and neighborhood-based methods. Each method has its strengths and can be combined or compared for a comprehensive anomaly detection strategy.\n\n**Final check for Anomaly detection**\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Applying Isolation Forest for anomaly detection\niso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=0)\nanomalies = iso_forest.fit_predict(scaled_numerical_data)\n\n# Adding the anomaly predictions to the PCA DataFrame\npca_df['Anomaly'] = anomalies\n\n# Visualizing the identified anomalies on the PCA plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', hue='Anomaly', data=pca_df, palette={1: 'blue', -1: 'red'})\nplt.title('PCA of Scaled Numerical Data with Anomalies Highlighted')\nplt.legend(title='Anomaly', loc='upper right', labels=['Normal', 'Anomaly'])\n\nplt.show()\n\n# Counting the number of detected anomalies\nnum_anomalies_detected = (anomalies == -1).sum()\n\nnum_anomalies_detected, pca_df.head()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=810 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(50,\n         PC1       PC2  Anomaly\n 0 -0.301194 -1.080589        1\n 1 -0.282737 -0.824039        1\n 2 -0.474600  2.067434        1\n 3  0.344241  0.305833        1\n 4 -0.009123  0.894454        1)\n```\n:::\n:::\n\n\n**Conclusion**\n\nIn conclusion, the analysis of anomalies and outliers in financial datasets is of paramount importance for safeguarding financial systems and ensuring data quality. In this report, we delved into the application of various techniques to detect anomalies in a financial dataset containing Age, Income, Transaction Amount, Transaction Type, and Location. The results of this analysis can guide decision-makers in identifying potential issues, such as fraudulent transactions or data entry errors.\n\nBy leveraging advanced analytics and machine learning algorithms, financial institutions can proactively mitigate risks, enhance customer trust, and streamline their operations. The continuous monitoring and refinement of anomaly detection methods are critical in the ever-evolving landscape of finance.\n\nAs financial data continues to grow in complexity and volume, the ability to detect anomalies accurately becomes increasingly crucial. It is essential for organizations to stay vigilant and adapt their anomaly detection strategies to stay ahead of emerging threats and challenges in the financial sector.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}