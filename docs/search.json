[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "In the past, I lost track of reality trying to track a gazillion links covering every data-science-friendly programming language under the sun. **shakes head** Bad idea. Since I program in R daily, I like to keep track of R and Posit / RStudio developments. I‚Äôm mostly going to share R resources that I find useful for analytics, statistical programming, machine learning, data science workflows, and web app development. I‚Äôm enjoying Python a lot more recently so I‚Äôll slowly build up this resources page with Python sub-topics that I find bookmark worthy.\nIn terms of the best place to start for getting into data analysis, I recommend learning SQL as this is by far the most widely used data querying language across the corporate and academic landscapes and if you master SQL, you‚Äôve mastered most of the transformations that are possible for tabular numeric data sets. Nonetheless, I will not cover SQL resources here as I rarely write raw SQL anymore. Instead, I use R to establish data warehouse connections and I query that raw data using the common tidyverse collection of R packages to execute SQL code in the back-end (via the dbplyr package).\nPython and R are open-source programming languages for statistical computing and graphics. These two languages have friendly online (and in-person) communities devoted to making data science easier to consume, easier to apply, and more effective at solving business problems. One of the things that I like most about both languages is the thousands of packages available making almost everything in R or Python a little easier from ETL, to method chaining, to developing predictive models and interactive web apps. I certainly welcome any suggestions that you might have for the lists below!"
  },
  {
    "objectID": "resources.html#language-agnostic-etl-frameworks",
    "href": "resources.html#language-agnostic-etl-frameworks",
    "title": "Resources",
    "section": "Language Agnostic ETL Frameworks",
    "text": "Language Agnostic ETL Frameworks\n\nArrow: Apache Arrow is a columnar memory format for flat and hierarchical data, organized for efficient analytic operations, supporting zero-copy reads for lightning-fast data access without serialization overhead\nDuckDB: DuckDB is an in-process SQL OLAP database management system (that plays nicely with Arrow) capable of larger than memory processing of tabular data\nPolars: Polars is a lightning fast DataFrame library/in-memory query engine written in Rust and built upon the Arrow specification - It‚Äôs a great tool for efficient data wrangling, data pipelines, snappy APIs and much more\nSpark: Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters"
  },
  {
    "objectID": "resources.html#python-books",
    "href": "resources.html#python-books",
    "title": "Resources",
    "section": "Python Books",
    "text": "Python Books\n\nThe Quick Python Book (3e): This book by Naomi Ceder is a few years old now (2018) but it‚Äôs the best end-to-end intro on Python that I‚Äôve yet read taking you from basic classes / structures to function writing to working with modules\nPython Data Science Handbook: Introduction to the core libraries essential for working with data in Python\nEffective Pandas: Patterns for Data Manipulation: Easy to follow tutorials, at your own pace, for mastering the popular Pandas library\nTidy Finance with Python: This is one of my favorite newer books covering complex financial modeling, valuation, and pricing and represents ‚Äúan opinionated approach to empirical research in financial economics [with an] open-source code base‚Äù"
  },
  {
    "objectID": "resources.html#python-packages",
    "href": "resources.html#python-packages",
    "title": "Resources",
    "section": "Python Packages",
    "text": "Python Packages\n\nNumPy: Brings the computational power of C and Fortran to Python programmers for applying high-level mathematical functions to arrays and more\nPandas: This is the most popular package for data manipulation and analysis with extended operations available for tabular and time series data\nMatplotlib: A comprehensive library for creating static, animated, and interactive visualizations in Python\nscikit-learn: Built on top of NumPy, SciPy, and matplotlib, ‚Äúsklearn‚Äù makes the development of predictive analysis workflows a simple and reproducible process\nBeautiful Soup: The beautifulsoup4 library makes web scraping HTML and XML data a breeze\nStreamlit: Using pure Python, this package lets you build interactive web apps in minutes with no UI / front-end experience required\nShiny for Python: The popular Shiny framework for R is finally available for Python - Create highly interactive visualizations, realtime dashboards, data explorers, model demos, sophisticated workflow apps, and anything else you can imagine‚Äîall in pure Python, with no web development skills required"
  },
  {
    "objectID": "resources.html#r-books-applied-resources",
    "href": "resources.html#r-books-applied-resources",
    "title": "Resources",
    "section": "R Books: Applied Resources",
    "text": "R Books: Applied Resources\n\nTidy Modeling with R: Over the last few months, I‚Äôve learned a lot from this A to Z resource on predictive modeling workflows using the tidymodels framework\nDeep Learning with R (2e): In-depth introduction to artificial intelligence and deep learning applications with R using the Keras library\nForecasting Principles and Practice (3e): Said best by the author, ‚ÄúThe book is written for three audiences: (1) people finding themselves doing forecasting in business when they may not have had any formal training in the area; (2) undergraduate students studying business; (3) MBA students doing a forecasting elective‚Äù\nRegression and Other Studies: Super applied textbook on advanced regression techniques, Bayesian inference, and causal inference\nSupervised Machine Learning for Text Analysis in R: Written by two Posit software engineers, Emil Hvitfeldt and Julia Silge, this book is a masterclass in natural language processing taking you from the basics of NLP to real-life applications including inference and prediction\nTidy Finance with R: This is one of my favorite newer books covering complex financial modeling, valuation, and pricing and represents ‚Äúan opinionated approach to empirical research in financial economics [with an] open-source code base in multiple programming languages‚Äù"
  },
  {
    "objectID": "resources.html#r-packages",
    "href": "resources.html#r-packages",
    "title": "Resources",
    "section": "R Packages",
    "text": "R Packages\n\ntidyverse: A collection of packages for data manipulation and functional programming (I use dplyr, stringr, and purrr on a daily basis)\ntidymodels: Hands-down my preferred collection of packages for building reproducible machine learning recipes, workflows, model tuning, model stacking, and cross-validation\ntidyverts: A collection of packages for time series analysis that comes out of Rob Hyndman‚Äôs lab\nDT: This is an R implementation of the popular DataTables JavaScript library that lets you build polished, configurable tables for use in web reports, slides, and Shiny apps\nbs4Dash: This R Shiny framework brings Bootstrap + AdminLTE dependencies to Shiny (including 1:1 support for shinydashboard functions) and it‚Äôs my go-to for developing enterprise-grade Shiny apps\nleaflet: R implementation of the popular Leaflet JavaScript library for developing interactive maps\nplotly: An extensive graphic library for creating interactive visualizations and 3D (WebGL) charts\nembed: This package is one of my go-to packages for machine learning and I if I‚Äôm working on a classification problem, you can count on me incorporating some of the extra steps it provides for the recipes package for embedding predictors into one or more numeric columns"
  },
  {
    "objectID": "posts/llm/llm.html",
    "href": "posts/llm/llm.html",
    "title": "Large Language Models (LLM)",
    "section": "",
    "text": "Large Language Models (LLMs) are a way to automatically generate text. more informative commit messages based on the currently staged changes.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/git/git.html",
    "href": "posts/git/git.html",
    "title": "Git",
    "section": "",
    "text": "Git is a version control system created by Linus Torvalds, who is also the creator of Linux. Since its first release, Git has largely replaced other version control systems and is used by 93% of software developers worldwide, according to survey results published by StackOverflow.\nThanks to Git, I can sleepüí§soundly at night, reassured by the knowledge that all of my progress on all of my ongoing project is recorded. My progress is not only stored on my local computer, but also on Git repository hosting services such as GitHub and GitLab.\nA Git repository (repo) is a directory that tracks the changes made to its contents. For more information on GitHub, the largest Git repo host, take a look at the ‚ÄúGitHub for supporting, reusing, contributing, and failing safely‚Äù post by Allison Horst and Julie Lowndes on the Openscapes blog.\nGitHub and GitLab do not only host repos, but also websites via GitHub Pages and GitLab Pages. My personal site is hosted for free on both GitHub Pages and GitLab Pages.\nBefore you can benefit from everything GitHub and/or GitLab have to offer, you will need to set up your computer so that you can work locally and/or configure a service like GitHub Codespaces or GitLab Web IDE so you can work remotely in your web browser. In this post, I discuss local development environment setup with the Homebrew package manager and repo setup with the GitHub and GitLab CLI.\nIf you follow along to end of this post, you will have a repo called dotfiles that can be used for Codespaces setup."
  },
  {
    "objectID": "posts/git/git.html#sec-setup",
    "href": "posts/git/git.html#sec-setup",
    "title": "Git",
    "section": "Setup",
    "text": "Setup\n\nHomebrew\nLinux, macOS, or Windows Subsystem for Linux (WSL) users can use Homebrew to install everything needed to work through all of the examples in this blog post. First, install Homebrew itself with the latest .pkg installer for macOS or by running the Example¬†1 Unix shell code in your terminal.\n\nExample 1 ¬†\n/bin/bash -c \"$(curl -fsSL https://\\\nraw.githubusercontent.com/Homebrew/\\\ninstall/HEAD/install.sh)\"\n\nIf you are not completely satisfied with the integrated terminal built into your preferred source-code editor or the standalone terminal that comes with your operating system (OS), you can use Homebrew to install a new one. The standalone terminal I use most often is iTerm2, which is only for macOS, but I also have the following multi-OS terminals: Alacritty, Hyper, Kitty, and Tabby.\nAfter installing Homebrew, you can run brew doctor in your terminal to confirm that everything is set up correctly. If the brew command is not available, you need to follow the instructions provided after installation to add brew to your PATH variable.\nOnce Homebrew is ready, you can run the shell code in Example¬†2 to create a file called Brewfile with the echo shell command and install everything listed in this newly created Brewfile with the brew bundle shell command.\n\nExample 2 ¬†\necho 'brew \"gh\"\\nbrew \"git\"\nbrew \"glab\"\\ncask \"github\"' &gt; Brewfile\nbrew bundle\n\nThe Brewfile created by the shell code in Example¬†2 installs:\n\nGit,\nthe command line interfaces (CLIs) for\n\nGitHub and\nGitLab, and\n\nthe GitHub Desktop Git Graphical User Interface (GUI).\n\nIf you are curious about how I set up my computer, you can take a look at my Brewfile and other configuration files in my setup repo on GitHub and GitLab. I will highlight a few configuration files in Section¬†2.\n\n\nRepository\nBefore we get started, you will need a GitHub and/or GitLab account and a way to authenticate into your account(s).\nOf the many authentication methods, passkeys stand out because they can function as both a password and two-factor authentication%20is%20an%20electronic%20authentication%20method%20in%20which%20a%20user%20is%20granted%20access%20to%20a%20website%20or%20application%20only%20after%20successfully%20presenting%20two%20or%20more%20pieces%20of%20evidence%20(or%20factors)%20to%20an%20authentication%20mechanism.) (2FA), thus combining the two steps in the 2FA sign-in process into one. Passkeys will certaily GitHub recently announced its plan to make 2FA mandatory for code contributors, which will make passkeys . SSH key.\nsatisfy both requirements, including SSH keys are still the easiest way to authenticate to GitHub and GitLab in my honest opinion.\nYou can create a repo using the web interface of https://github.com or https://gitlab.com in your browser, but the best way to start a new project is using the CLI for GitHub or GitLab in your terminal. First, run gh auth login or glab auth login and follow the prompts to authenticate via your web browser or with an authentication token.\nThe GitHub CLI allows you to add an SSH key to your account during or after authentication. The GitLab CLI does not handle SSH keys during authentication, but has a similar command for adding an SSH key to your GitLab account.\nAfter authentication and SSH key setup, you can run the code in either of the code chunks in Example¬†3 to set up your local and remote repos and create a Quarto website project in the local repo. You can create shell alias that combine all of the repo creation steps like I did in my .zshrc.\n\nExample 3 ¬†\n\nGitHubGitLab\n\n\ncd # start in home directory\nmkdir USERNAME\ncd USERNAME\ngh repo create REPONAME --add-readme --clone --public\ncd REPONAME\n\n\ncd # start in home directory\nmkdir USERNAME\ncd USERNAME\nglab repo create REPONAME --readme --defaultBranch main --public\ncd REPONAME\ngit pull origin main\ngit branch --set-upstream-to=origin/main main\n\n\n\n\nTo make it easier to backup my repos on both GitHub and GitLab, I set up each local repo to have two origin remote URLs using the code as shown in Example¬†4. With this setting, running git push in my local repo updates my remote repos on both GitHub and GitLab.\n\nExample 4 ¬†\ngit remote add lab git@gitlab.com:maptv/maptv.gitlab.io\ngit remote add hub git@github.com:maptv/maptv.github.io\ngit remote set-url --add origin $(git remote get-url lab)"
  },
  {
    "objectID": "posts/git/git.html#sec-workflow",
    "href": "posts/git/git.html#sec-workflow",
    "title": "Git",
    "section": "Git workflow",
    "text": "Git workflow\nWhen I want to add or update the content on my site, I go through the steps in the standard Git workflow shown in Figure¬†1. Every time I ‚Äúpush‚Äù a collection of changes called a commit to my GitHub repo on GitHub, a continuous integration (CI) system called GitHub Actions automatically completes the steps required to build and publish my website.\n\nflowchart LR\n   A[working&lt;br/&gt;directory]-.git&lt;br/&gt;add.-&gt;B{{staging&lt;br/&gt;area}}-.git&lt;br/&gt;commit.-&gt;C([local&lt;br/&gt;repo])-.git&lt;br/&gt;push.-&gt;D(remote&lt;br/&gt;repo)\n\n\n\n\nflowchart LR\n   A[working&lt;br/&gt;directory]-.git&lt;br/&gt;add.-&gt;B{{staging&lt;br/&gt;area}}-.git&lt;br/&gt;commit.-&gt;C([local&lt;br/&gt;repo])-.git&lt;br/&gt;push.-&gt;D(remote&lt;br/&gt;repo)\n\n\nFigure¬†1: Git workflow\n\n\n\n\n\nShell aliases\nTo make it easier to make incremental changes to my website and frequently release new content, I combined all of the git shell commands in Figure¬†1 into shell aliases. You can add shell aliases to a shell configuration file like .bashrc or .zshrc on your computer to shorten one or more commands and any associated command arguments.\nThe git commit aliases in the .zshrc file in my setup repo on GitHub and GitLab target different groups of files for inclusion in the next commit. For example, cmp targets staged files, camp targets tracked files, a.cmp targets files in the current directory, and aacmp targets files in the repo.\nExample¬†5 shows the aacmp alias as an example of the shell alias syntax. The mnemonic device for this alias is add all, commit with a message, and push.\n\nExample 5 ¬†\n\n\n.zshrc\n\nalias aacmp=\"func() { git add --all && git commit --message \\\n    \\\"$(echo '${*:-$(echo $(git diff --name-status --cached \\\n    | tr \"[:space:]\" \" \"))}')\\\" && git push; }; func\"\n\n\nAliases like aacmp allow me to enter free-form commit messages directly on the command line without quotes, e.g.¬†aacmp edit first post. If you decide to try one of these aliases, please exercise extreme caution as any un-escaped or un-quoted metacharacters may yield surprising effects instead of being included verbatim in the commit message, e.g.¬†* is replaced by all of the file and directory names in the current directory!\nIf no commit message is provided after the aliases, a generic commit message is created that includes the change type and name of each changed file. In Section¬†2.2, I describe how I used this generic commit message approach to further simplify the Git workflow.\n\n\nKeybindings\nAn alternative to a shell alias that combines git commands is to use a keyboard shortcut in a Git Graphical User Interface (GUI) such as GitHub Desktop or the Git interface in a code editor like VSCode, VSCodium, or RStudio. I use keyboard shortcuts in VSCode and VSCodium to send shell commands to the integrated terminal without moving my focus away from the files I am editing.\nI created different shortcuts to control which files are included in each commit: ‚å•‚áßF for the current file only, ‚å•‚áßS for already staged files, ‚å•‚áßT for all tracked files, and ‚å•‚áßU for all files including untracked files. I also have keyboard shortcuts that affect a specific directory (and all of its subdirectories): ‚å•‚áßD for the current file‚Äôs directory, ‚å•‚áß. for shell‚Äôs current directory, ‚å•‚áßC for the current working directory according to VSCode/VSCodium, ‚å•‚áßW for the Workspace directory.\nExample¬†6 displays the ‚å•‚áßF shortcut as an example of the VSCode/VSCodium shortcut syntax. This shortcut uses the escape code for the return key (\\u000D) to run several git commands and predefined variables to insert the absolute (${file}) and relative (${relativeFile}) path to the currently open file.\n\nExample 6 ¬†\n\n\nkeybindings.json\n\n{\n  \"key\": \"shift+alt+f\",\n  \"command\": \"workbench.action.terminal.sendSequence\", \"args\": { \"text\":\n    \"git add ${file} && git commit -m \\\"M ${relativeFile}\\\" && git push\\u000D\" },\n  \"when\": \"terminalIsOpen\"\n}\n\n\nIf you want to set up similar shortcuts for yourself, take a look at my keybindings.json in my setup repo on GitHub and GitLab. As you create keyboard shortcuts, please be mindful of keybinding conflicts that may arise.\nTo set up a keyboard shortcut that runs a series of steps rather than a single line of shell code, I suggest you use the VSCode/VSCodium Tasks mechanism, a system designed to automate software build tasks. The default keyboard shortcut to run all tasks in a local or global task.json file is ‚åÉ‚áßB on Linux/Windows or ‚åò‚áßB on Mac (mnemonic: B is for Build), but you can bind other shortcuts to specific tasks.\nIf you use a text editor like Vim or Emacs, you can create keybindings for Vim plugins like fugitive or Emacs packages like magit that run through the entire Git workflow. Example¬†7 shows the Vim+fugitive equivalent of my ‚å•‚áßF VSCode/VSCodium keybinding.\n\nExample 7 ¬†\n\n\n.vimrc\n\nnnoremap &lt;A-S-f&gt; :Gw&lt;bar&gt;G! commit -m \"M \"%&lt;bar&gt;G! push&lt;CR&gt;\n\n\nThe drawback of my keyboard shortcut approach for the Git workflow is that it produces generic commit messages that are no very informative. Anyone reading the messages will not be able to tell what changes were made and more importantly why the changes were made.\nTo automatically generate commit messages based on the currently staged changes, we can use a Large Language Model (LLM). Generative artificial intelligence models like LLMs tend to be large in size and have atypical computational requirements.\nI really enjoy using Git, especially with shell aliases in my terminal and keyboard shortcuts in my favorite text editors. If we ever collaborate on a project together, you can be sure that I will insist on using Git!"
  },
  {
    "objectID": "posts/Classification/index.html#implementing-advanced-classification-techniques-for-precise-model-prediction-analysis-to-enhance-accuracy-and-efficiency",
    "href": "posts/Classification/index.html#implementing-advanced-classification-techniques-for-precise-model-prediction-analysis-to-enhance-accuracy-and-efficiency",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Implementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency**",
    "text": "Implementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency**\nIntroduction\nMachine learning is a fascinating field that empowers computers to learn and make predictions or decisions without being explicitly programmed. One of the fundamental tasks in machine learning is classification, where the goal is to categorize data points into predefined classes or labels. In this blog post, we dive deep into the world of classification, exploring advanced techniques and their application on a real-world dataset.\nThe Iris dataset is a well-known benchmark in the machine learning community. It consists of measurements of four features from three different species of iris flowers. This seemingly simple dataset serves as an excellent playground for understanding and implementing classification algorithms. However, we won‚Äôt stop at the basics; we‚Äôll explore advanced classification techniques, model tuning, and even dive into ensemble methods and neural networks.\nData Loading and Preliminary Analysis\n\n# Importing essential libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Load dataset\niris_df = pd.read_csv('Iris_dataset.csv')\n\n# Basic dataset information\nprint(iris_df.head())\nprint(iris_df.describe())\nprint(iris_df.info())\n\n# Visualizing the distribution of classes\nsns.countplot(x='species', data=iris_df)\nplt.show()\n\n# Pairplot to explore relationships between features\nsns.pairplot(iris_df, hue='species')\nplt.show()\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0           6.370695          2.771952           5.118790          1.542084   \n1           5.979286          2.612095           5.086595          1.560228   \n2           6.741563          2.804321           4.758669          1.443702   \n3           6.346538          2.796799           5.601084          2.114922   \n4           5.558280          2.831451           4.876331          2.045125   \n\n      species  \n0   virginica  \n1  versicolor  \n2  versicolor  \n3   virginica  \n4   virginica  \n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount        3000.000000       3000.000000        3000.000000   \nmean            5.844003          3.055776           3.756746   \nstd             0.825203          0.435899           1.761100   \nmin             4.180767          1.918769           0.907839   \n25%             5.118694          2.780525           1.559288   \n50%             5.777010          3.025634           4.347984   \n75%             6.416866          3.342169           5.098831   \nmax             7.963257          4.516746           7.046886   \n\n       petal width (cm)  \ncount       3000.000000  \nmean           1.199022  \nstd            0.760822  \nmin           -0.048888  \n25%            0.310048  \n50%            1.326659  \n75%            1.818514  \nmax            2.601413  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  3000 non-null   float64\n 1   sepal width (cm)   3000 non-null   float64\n 2   petal length (cm)  3000 non-null   float64\n 3   petal width (cm)   3000 non-null   float64\n 4   species            3000 non-null   object \ndtypes: float64(4), object(1)\nmemory usage: 117.3+ KB\nNone\n\n\n\n\n\n\n\n\nData Preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Encoding categorical data\nencoder = LabelEncoder()\niris_df['species'] = encoder.fit_transform(iris_df['species'])\n\n# Splitting dataset into features and target variable\nX = iris_df.drop('species', axis=1)\ny = iris_df['species']\n\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nExploratory Data Analysis (EDA)\n\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(iris_df.corr(), annot=True, cmap='viridis')\nplt.show()\n\n# Advanced pairplot with distribution and regression\nsns.pairplot(iris_df, kind='reg', hue='species')\nplt.show()\n\n\n\n\n\n\n\nModel Building and Evaluation For Classification\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Function to train and evaluate models\ndef train_evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(f'Model: {model.__class__.__name__}')\n    print(classification_report(y_test, predictions))\n    # Confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    sns.heatmap(cm, annot=True)\n    plt.show()\n\n# Decision Tree Classifier\ntrain_evaluate_model(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)\n\n# RandomForestClassifier\ntrain_evaluate_model(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n\n# GradientBoostingClassifier\ntrain_evaluate_model(GradientBoostingClassifier(), X_train, y_train, X_test, y_test)\n\n# Support Vector Machine (SVC)\ntrain_evaluate_model(SVC(), X_train, y_train, X_test, y_test)\n\n# K-Nearest Neighbors (KNN)\ntrain_evaluate_model(KNeighborsClassifier(), X_train, y_train, X_test, y_test)\n\n# Logistic Regression\ntrain_evaluate_model(LogisticRegression(), X_train, y_train, X_test, y_test)\n\nModel: DecisionTreeClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.98      0.98       299\n           2       0.98      0.99      0.99       312\n\n    accuracy                           0.99       900\n   macro avg       0.99      0.99      0.99       900\nweighted avg       0.99      0.99      0.99       900\n\nModel: RandomForestClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: GradientBoostingClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: SVC\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.96      0.98       299\n           2       0.97      0.99      0.98       312\n\n    accuracy                           0.98       900\n   macro avg       0.99      0.98      0.98       900\nweighted avg       0.98      0.98      0.98       900\n\nModel: KNeighborsClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       1.00      1.00      1.00       299\n           2       1.00      1.00      1.00       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: LogisticRegression\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.98      0.95      0.97       299\n           2       0.96      0.98      0.97       312\n\n    accuracy                           0.98       900\n   macro avg       0.98      0.98      0.98       900\nweighted avg       0.98      0.98      0.98       900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code, we introduce different types of classification models, including Random Forest Classifier, Gradient Boosting Classifier, Support Vector Classifier (SVC), K-Nearest Neighbors Classifier (KNN), and Logistic Regression.\nFor each model, we train it on the training data and evaluate its performance using accuracy and a classification report that includes precision, recall, and F1-score. This allows you to compare the performance of various classification algorithms on the Iris dataset.\nAdvanced Model Tuning and Analysis\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for RandomForestClassifier\nparam_grid = {'n_estimators': [10, 50, 100], 'max_features': ['sqrt', 'log2', None]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Evaluating the tuned model\ntrain_evaluate_model(best_rf, X_train, y_train, X_test, y_test)\n\nModel: RandomForestClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\n\n\n\n\n\nHyperparameter Tuning for the Classification Model\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Proper Hyperparameter tuning for Random Forest Classifier\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\ngrid_search_rf.fit(X_train, y_train)\n\nbest_rf_classifier = grid_search_rf.best_estimator_\n\n# Evaluating the tuned Random Forest Classifier\ntuned_rf_predictions = best_rf_classifier.predict(X_test)\nprint(\"Tuned Random Forest Classifier - Model Evaluation\")\nprint(\"Accuracy:\", accuracy_score(y_test, tuned_rf_predictions))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, tuned_rf_predictions))\n\nTuned Random Forest Classifier - Model Evaluation\nAccuracy: 0.9955555555555555\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\n\n\nROC Curve Analysis for Multiple Models\nComparing the performance of the various classification models using ROC Curve analysis, here we discuss the plots of the ROC Curve for each model. This will involve calculating the True Positive Rate (TPR) and False Positive Rate (FPR) for each model and plotting them.\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nimport matplotlib.pyplot as plt\n\n# Binarize the output classes for ROC analysis\ny_bin = label_binarize(y, classes=[0, 1, 2])\nn_classes = y_bin.shape[1]\n\n# Splitting the data again for multiclass ROC analysis\nX_train, X_test, y_train_bin, y_test_bin = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n\n# Classifier list\nclassifiers = [\n    OneVsRestClassifier(DecisionTreeClassifier()),\n    OneVsRestClassifier(RandomForestClassifier()),\n    OneVsRestClassifier(GradientBoostingClassifier()),\n    OneVsRestClassifier(SVC(probability=True)),\n    OneVsRestClassifier(KNeighborsClassifier()),\n    OneVsRestClassifier(LogisticRegression())\n]\n\n# Plotting ROC Curves\nplt.figure(figsize=(10, 8))\n\n# Compute ROC curve and ROC area for each class\nfor classifier in classifiers:\n    classifier.fit(X_train, y_train_bin)\n    y_score = classifier.predict_proba(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f'{classifier.estimator.__class__.__name__} (area = {roc_auc:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nThis code will generate ROC curves for each of the classifiers used, providing a visual comparison of their performance in terms of the trade-off between the True Positive Rate and False Positive Rate. The area under the curve (AUC) is also displayed as a measure of the model‚Äôs performance, with a higher AUC indicating a better model. This analysis is crucial for understanding the performance of classification models, especially in multi-class settings.\nDimensionality Reduction and Visualization\n\nfrom sklearn.decomposition import PCA\n\n# PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Visualizing PCA results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', label='Train set')\nplt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='plasma', label='Test set', marker='x')\nplt.xlabel('First principal component')\nplt.ylabel('Second principal component')\nplt.legend()\nplt.title('PCA of Iris Dataset')\nplt.show()\n\n\n\n\nAdditional Advanced Analysis\n1. Cross-Validation\nCross-validation is a technique used to evaluate the generalizability of a model by training and testing it on different subsets of the dataset.\n\nfrom sklearn.model_selection import cross_val_score\n\n# Example using RandomForestClassifier\nrf_classifier = RandomForestClassifier()\n\n# Performing 10-fold cross-validation\ncv_scores = cross_val_score(rf_classifier, X, y, cv=10)\n\nprint(\"Cross-Validation Scores for RandomForestClassifier:\", cv_scores)\nprint(\"Average Score:\", np.mean(cv_scores))\n\nCross-Validation Scores for RandomForestClassifier: [0.99666667 1.         0.99333333 0.99333333 1.         0.99666667\n 0.99666667 0.99666667 1.         1.        ]\nAverage Score: 0.9973333333333333\n\n\n2. Ensemble Methods\nEnsemble methods combine multiple models to improve the overall performance. Here, I will use an ensemble of different classifiers.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\n# Update Logistic Regression in the ensemble\nclassifiers = [\n    ('Decision Tree', DecisionTreeClassifier()),\n    ('Random Forest', RandomForestClassifier()),\n    ('Gradient Boosting', GradientBoostingClassifier()),\n    ('SVC', SVC(probability=True)),\n    ('KNN', KNeighborsClassifier()),\n    ('Logistic Regression', LogisticRegression(max_iter=1000))\n]\n\nensemble = VotingClassifier(estimators=classifiers, voting='soft')\nensemble.fit(X_train, y_train)\nensemble_predictions = ensemble.predict(X_test)\n\nprint(\"Ensemble Model Classification Report:\")\nprint(classification_report(y_test, ensemble_predictions))\n\nEnsemble Model Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       1.00      0.99      0.99       299\n           2       0.99      1.00      1.00       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\n\n\nPREDICTION\nPrediction: Data Preprocessing with Visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Encoding categorical data\nencoder = LabelEncoder()\niris_df['species'] = encoder.fit_transform(iris_df['species'])\n\n# Visualizing the distribution of the target variable\nplt.figure(figsize=(8, 5))\nsns.countplot(x='species', data=iris_df)\nplt.title('Distribution of Target Variable (Species)')\nplt.show()\n\n# Splitting dataset into features and target variable\nX = iris_df.drop('species', axis=1)\ny = iris_df['species']\n\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Pairplot to explore relationships between features\npairplot_df = iris_df.copy()\npairplot_df['species'] = encoder.inverse_transform(pairplot_df['species'])\n\nplt.figure(figsize=(10, 8))\nsns.pairplot(pairplot_df, hue='species')\nplt.title('Pairplot of Features with Species as Hue')\nplt.show()\n\n\n\n\n&lt;Figure size 960x768 with 0 Axes&gt;\n\n\n\n\n\nModel Building, Evaluation, and Visualization for Prediction\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\n\n# Function to train and evaluate regression models\ndef train_evaluate_regression_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    \n    # Model Evaluation\n    print(f'Model: {model.__class__.__name__}')\n    print(f'Mean Squared Error: {mean_squared_error(y_test, predictions)}')\n    print(f'R-squared (R2) Score: {r2_score(y_test, predictions)}')\n    \n    # Visualization\n    plt.figure(figsize=(8, 5))\n    plt.scatter(y_test, predictions)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title(f'{model.__class__.__name__} - True vs. Predicted Values')\n    plt.show()\n\n# Linear Regression\ntrain_evaluate_regression_model(LinearRegression(), X_train, y_train, X_test, y_test)\n\n# Decision Tree Regressor\ntrain_evaluate_regression_model(DecisionTreeRegressor(), X_train, y_train, X_test, y_test)\n\n# Random Forest Regressor\ntrain_evaluate_regression_model(RandomForestRegressor(), X_train, y_train, X_test, y_test)\n\nModel: LinearRegression\nMean Squared Error: 0.047703003079178956\nR-squared (R2) Score: 0.9284946222241109\nModel: DecisionTreeRegressor\nMean Squared Error: 0.01\nR-squared (R2) Score: 0.9850102984801183\nModel: RandomForestRegressor\nMean Squared Error: 0.006380666666666666\nR-squared (R2) Score: 0.9904355711168809\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Model Tuning and Analysis for Prediction\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for Random Forest Regressor\nparam_grid = {\n    'n_estimators': [10, 50, 100],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\nbest_rf = grid_search.best_estimator_\n\n# Evaluating the tuned model\npredictions = best_rf.predict(X_test)\nprint(\"Tuned Random Forest Regressor - Model Evaluation\")\nprint(f'Mean Squared Error: {mean_squared_error(y_test, predictions)}')\nprint(f'R-squared (R2) Score: {r2_score(y_test, predictions)}')\n\nTuned Random Forest Regressor - Model Evaluation\nMean Squared Error: 0.005469777777777778\nR-squared (R2) Score: 0.9918009663731029\n\n\nFeature Selection and Importance Analysis\nFeature selection is crucial for improving model performance and reducing overfitting. Here, we use techniques like Recursive Feature Elimination (RFE) and feature importance analysis to select the most relevant features for classification or prediction.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Using Recursive Feature Elimination (RFE) with Logistic Regression\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select=3)  # Select the top 3 features\nfit = rfe.fit(X_train, y_train)\n\n# List of selected features\nselected_features = [feature for idx, feature in enumerate(X.columns) if fit.support_[idx]]\nprint(\"Selected Features:\", selected_features)\n\nSelected Features: ['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\nAdditionally, we can analyze feature importance for tree-based models like Random Forest or Gradient Boosting to understand which features contribute the most to predictions.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Feature Importance Analysis for Random Forest Classifier\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\n\n# Plot feature importance\nfeature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\nfeature_importance.nlargest(5).plot(kind='barh')\nplt.title(\"Feature Importance (Random Forest)\")\nplt.show()\n\n\n\n\nHandling Class Imbalance\nIn real-world datasets, class imbalance is common, where one class has significantly fewer samples than others. Techniques like oversampling, undersampling, and Synthetic Minority Over-sampling Technique (SMOTE) can be employed to address this issue.\n\nfrom imblearn.over_sampling import SMOTE\n\n# Using SMOTE to handle class imbalance\nsmote = SMOTE(sampling_strategy='auto')\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nConclusion\nIn this journey through the Iris dataset and the realm of classification, we‚Äôve covered a wide range of topics. Starting with data loading and preprocessing, we explored the relationships between features, ensuring that we understood our data thoroughly. We then introduced a variety of classification models, from decision trees to support vector machines, and compared their performance using robust evaluation metrics.\nBut we didn‚Äôt stop there. We delved into advanced techniques, including cross-validation to ensure the generalizability of our models, ensemble methods that combined the strengths of multiple classifiers, and even a taste of neural networks for classification tasks.\nOur exploration of ROC curves allowed us to visualize and compare the trade-offs between true positive and false positive rates across different models, providing valuable insights into their performance.\nIn the end, classification is a powerful tool in the machine learning toolkit, with applications ranging from medical diagnosis to spam email filtering. The Iris dataset served as an ideal playground to learn and experiment with these techniques, but the knowledge gained can be applied to more complex and real-world classification problems.\nAs you continue your journey in machine learning, remember that classification is just the tip of the iceberg. The world of machine learning is vast and ever-evolving, and there are countless exciting challenges and opportunities awaiting those who dare to explore it further."
  },
  {
    "objectID": "posts/2023-01-01-data-science-hangout/2023-01-01-data-science-hangout.html",
    "href": "posts/2023-01-01-data-science-hangout/2023-01-01-data-science-hangout.html",
    "title": "Data Science Hangout",
    "section": "",
    "text": "Background\nI‚Äôve been attending Posit‚Äôs weekly Data Science Hangout since 2021 and it‚Äôs been one of my favorite standing weekly meetings. This is a great platform to hear how data science leaders are using R to drive business results and the host, Rachael Dempsey, does an excellent job fostering an inclusive and open culture. I was thrilled to be the guest several weeks ago in Dec 2022. üöÄ\nI talked about my transition to data science from a career in financial modeling and consulting, talked about my love for Shiny and the R community, and gave some tips on how to start your own data science community. I also said ‚Äúyou know‚Äù an inordinate number of times, please forgive me in advance. üôÉ\nWhether you‚Äôre a data science practitioner, student, looking to make a career pivot, or simply wanting to network, the Data Science Hangout has something for all. If you haven‚Äôt already, I highly encourage you to attend Posit‚Äôs Data Science Hangout live sometime in the future.\n\n\nWatch on YouTube\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "href": "posts/2021-05-03-DT/2021-05-03-DT.html",
    "title": "DT: An R Interface to the JavaScript library DataTables",
    "section": "",
    "text": "The DT package for R is a powerful integration of the JavaScript library DataTables. One of the things that I enjoy with DT is enhancing my Shiny apps by rendering interactive tables with DT. The DT documentation, written by RStudio PBC and the DT devs, is incredibly well-written and you‚Äôll learn a lot about how you can use these tables in the development of robust R web apps. And the available Options‚Ä¶. Endless in a good way!\nIf you‚Äôre a Shiny dev, check out the live demos sourced below.\n\nSource:\n\nDT on GitHub\nDT site\nLive Shiny demo: DT Selections Ex. 1\nLive Shiny demo: DT Selection Ex. 2\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "href": "posts/2020-09-13-R-Package-Detailed-Tutorial/2020-09-13-R-Package-Detailed-Tutorial.html",
    "title": "Detailed R Package Development Tutorial from Method Bites",
    "section": "",
    "text": "I‚Äôll keep this short since all credit should go towards the amazing Cosima Meyer and Dennis Hammerschmidton‚Ä¶ They compressed some of the most valuable steps for creating and deploying a package. This duo‚Äôs work was published for the MZES Social Science Data Lab. If you want a detailed and easy-to-follow how-to for R package development, bookmark this step-by-step guide!\n\nSource: * Methods Bites: How to write your own R package and publish it on CRAN\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "href": "posts/2020-02-01-k-fold-cross-validation/2020-02-01-k-fold-cross-validation.html",
    "title": "Resampling with k-fold Cross Validation",
    "section": "",
    "text": "k-fold ‚Äúrandomly divides the training data into k groups (or folds) of approximate size [with the model being] fit on k-1 folds and the remaining fold used to compute model performance.‚Äù I use k=5 or k=10 given that this is typical industry practice without really questioning ‚Äúwhy?‚Äù\n\nIn reading through Hands-On Machine Learning with R by Brad Boehmke, Ph.D.¬†and Brandon Greenwell, I was surprised to learn that studies have shown that k=10 performs similarly to leave-one-out cross validation where k=n.\nWithout realizing it, sometimes I get carried away optimizing code and drifting from statistics and the core ‚Äúscience‚Äù in data science‚Ä¶ k-fold CV is a technique used by many and is agnostic to your statistical programming language of choice but if you‚Äôre an #R user, I can‚Äôt recommend this book enough (free in full online, link below)!\nSource:\n\nHands-On Machine Learning with R Chapter 2.4 Resampling Methods\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "href": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/2019-12-17-Hands-On-Machine-Learning-with-R.html",
    "title": "Book Rec: Hands-On Machine Learning with R",
    "section": "",
    "text": "I am super excited to finally dig into Hands-On Machine Learning with R by Brad Boehmke, Ph.D. and Brandon Greenwell.\nThere are a ton of solid ML books on the market for Python and R users but I‚Äôve struggled with developing a best practice ML workflow to make maintainable code and to enhance my sampling, evaluation, and iteration process. As an R user, I‚Äôm glad to have this hands-on approach resource to improve my use of the ML stack within R.\nThe full text and code samples are available online, for free: https://lnkd.in/gvmXY3W\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nishant Bharali",
    "section": "",
    "text": "Greetings! My name is Nishant Bharali.\nI‚Äôm an engineer with experiences in software modeling, analytics, vehicle controls, and machine learning. For a living, I get to solve problems with data‚Ä¶ Life is good!"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "Nishant Bharali",
    "section": "Programming",
    "text": "Programming\nI use Python, Java, MATLAB and C++ to develop and deploy packages, web apps, automation pipelines, machine learning workflows, and websites (this site was built with Quarto). My daily toolkit also includes Python."
  },
  {
    "objectID": "index.html#web-app-development",
    "href": "index.html#web-app-development",
    "title": "Nishant Bharali",
    "section": "Web App Development",
    "text": "Web App Development\nMy framework of choice for web app development is Java + Spring + React + Jenkins. With Spring, React and Jenkins, I can build enterprise-grade UIs on top of Bootstrap 5 that can be infinitely styled with HTML, Sass, CSS, JavaScript, and more."
  },
  {
    "objectID": "index.html#data-science-communities",
    "href": "index.html#data-science-communities",
    "title": "Nishant Bharali",
    "section": "Data Science Communities",
    "text": "Data Science Communities\nThere are a plethora of online and in-person data science communities to learn from and share your own experiences. Online communities for open-source languages and data science are incredibly welcoming and below are my favorites:\n\nSoCal R Users Group\nData Science Hangout\nR-Ladies Global\nThe Ravit Show"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Ubuntu Update to 19.10\n\n\n\n\n\n\n\nlinux\n\n\n\n\nThings not to do with Linux, learned the hard way, as we approach the end of 2019‚Ä¶\n\n\n\n\n\n\nDec 2, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nBook Rec: Hands-On Machine Learning with R\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDigging into Hands-On Machine Learning with R by Brad Boehmke, PhD, and Brandon Greenwell.\n\n\n\n\n\n\nDec 1, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection in a Financial Dataset\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDetecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Classification techniques with Model Prediction Analysis\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nImplementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Clustering and Prediction techniques\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nEmploying sophisticated clustering techniques with retail customer data and comparing further with prediction models\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nGit\n\n\n\n\n\n\n\nTools\n\n\n\n\nDiving deep into the Git tool\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced analysis on Linear and Non-Linear Regression models\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nA Comprehensive Analysis on Housing Market. Utilizing regression methodologies for the purpose of analyzing trends within the housing market\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nLarge Language Models (LLM)\n\n\n\n\n\n\n\nTools\n\n\n\n\nGaining insights on weather data with Data Science and Machine Learning\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Processes\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nGaining insights on weather data with Data Science and Machine Learning\n\n\n\n\n\n\nNov 24, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Website with Blogdown\n\n\n\n\n\n\n\nrmarkdown\n\n\nblogdown\n\n\n\n\nLearn how to build and deploy a website with R, blogdown, GitHub, Hugo, and Netlify.\n\n\n\n\n\n\nNov 18, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nDetailed R Package Development Tutorial from Method Bites\n\n\n\n\n\n\n\npackages\n\n\n\n\nThis is one of the best, most detailed, how-to guides for developing your own R package from A-to-Z\n\n\n\n\n\n\nSep 13, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nD&D Adventures + blogdown\n\n\n\n\n\n\n\nrmarkdown\n\n\nblogdown\n\n\n\n\nIt‚Äôs an incredible time to be creative! I‚Äôve recently been re-learning Dungeons & Dragons with a group of friends and having so much fun in the process.\n\n\n\n\n\n\nApr 5, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nResampling with k-fold Cross Validation\n\n\n\n\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\n\nClick for a high-level recap of k-fold cross validation as a resampling method.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nData Science Hangout\n\n\n\n\n\n\n\ntidyverse\n\n\nmachine learning\n\n\n\n\nI was recently a guest on Posit‚Äôs Data Science Hangout, hosted by Rachael Dempsey, and I had a blast. This is the recording where I talked about my transition from Excel-based financial modeling to machine learning at scale with R.\n\n\n\n\n\n\nJan 1, 2023\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nA Brief History of the Dataframe\n\n\n\n\n\n\n\ntibble\n\n\n\n\nInsightful read sourced from Towards Data Science on the history of the dataframe - From its origins in the S programming language, to R, to pandas for Python (to the tibble for R).\n\n\n\n\n\n\nMay 24, 2021\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nDT: An R Interface to the JavaScript library DataTables\n\n\n\n\n\n\n\ndt\n\n\n\n\nThe R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, interactivity with Shiny, and many other features.\n\n\n\n\n\n\nMay 3, 2021\n\n\nNishant Bharali\n\n\n\n\n\n\n  \n\n\n\n\nPreparing for 2021 Goals with a Raspberry Pi\n\n\n\n\n\n\n\nlinux\n\n\n\n\nI have been planning my 2023 goals and the Raspberry Pi 4 will help me kill a few birds with one stone.\n\n\n\n\n\n\nDec 31, 2020\n\n\nNishant Bharali\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "The sound of the Gion Shoja bells echoes the impermanence of all things; the color of the sala flowers reveals the truth that the prosperous must decline"
  },
  {
    "objectID": "about.html#professional",
    "href": "about.html#professional",
    "title": "About Me",
    "section": "Professional",
    "text": "Professional\nMy career includes over 2 years of software engineering for Oracle Cerner and automotive software development and management for Mahindra & Mahindra. I use the MATLAB, JAVA, Javascript and Python programming languages on a daily basis but am much more advanced with my MATLAB work. I help bridge the gap between business needs and IT to develop and deploy automation solutions, predictive algorithms, and interactive web apps during my time at Oracle Cerner and Mahindra."
  },
  {
    "objectID": "about.html#programming-analytics-skills",
    "href": "about.html#programming-analytics-skills",
    "title": "About Me",
    "section": "Programming & Analytics Skills",
    "text": "Programming & Analytics Skills\n\nOther Analytics & Programming Platforms: Python, Jupyter Notebooks, h2o, GitLab / GitHub, Google Cloud Platform, SQL, Netlify\nMicrosoft: Expert-level Excel financial modeler with experiences including nested formulas, XLOOKUPs, Solver for optimization and simulations (yes‚Ä¶ in Excel!), macros for psuedo-automated data manipulation, add-in integrations, and more"
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "nishantkb@vt.edu"
  },
  {
    "objectID": "cv/cv.html#summary",
    "href": "cv/cv.html#summary",
    "title": "Curriculum Vitae",
    "section": "Summary",
    "text": "Summary\nMy goal is to train and lead the next generation of data scientists to integrate substantive expertise from diverse fields with machine intelligence.\nThrough research and projects, I am constantly improving my ability to obtain, tidy, explore, transform, visualize, model, and communicate data. My preferred tools are MATLAB, Simulink, Java, JavaScript and Python, open source programming languages kept on the cutting edge by their energetic communities.\nIn addition to their utility in common data science tasks, these languages serve as accessible interfaces to deep learning libraries like TensorFlow.\nI aim to leverage my background in science and engineering to help shape the future of autonomy, controls and machine vision and deliver on the promise of machine learning applications."
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nVirginia Polytechnic Institute and State University, Blacksburg, Virginia 24060, USA\n\nAnticipated Graduation: May, 2025\nMaster of Science in Computer Engineering\n\nVellore Institute of Technology, Vellore, Tamil Nadu, India\n\n2018 ‚Äì 2022\nBachelor of Technology in Electronics and Communication Engineering\n\nCGPA: 3.33 / 4.00"
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE"
  },
  {
    "objectID": "cv/cv.html#product-development-engineer-mahindra-mahindra-limited-chennai-tn-india",
    "href": "cv/cv.html#product-development-engineer-mahindra-mahindra-limited-chennai-tn-india",
    "title": "Curriculum Vitae",
    "section": "Product Development Engineer | Mahindra & Mahindra Limited | Chennai, TN, India",
    "text": "Product Development Engineer | Mahindra & Mahindra Limited | Chennai, TN, India\nAugust 2022 ‚Äì July 2023\nR&D Engineer - Electrical & Electronics - CAN BUS Analysis of Electronic Control Units [Automotive Product Development]\n\nProjects: Worked on the infotainment and cluster software architecture for ICE & BEV vehicles; Mahindra Scorpio N Z6/Z8 variants, XUV700 & XUV400 EV, providing bench-level as well as vehicle level environmental testing and production.\nCollaborated with cross-functional teams like ADAS, HMI and wiring harness to incorporate architecture changes.\nLed and completed 2 core Android Auto certifications for mentioned projects with software testing on various levels - PCTS Verifier, QSuite, UX, GPS/Navigation, sensor logging and final software validation; resulting in 15% improvement in sales.\nPrepared SRD (Software Requirements Definition) as a combined activity with the supplier; Improved data management processes by 12%.\nAuthored technical documentation, design specifications, test plans, & streamline project development; Integrated OTA updates, GPS benchmark and connectivity features for enhanced user experiences.\nExperienced in writing test cases from system requirements and customer use cases.\nCollaborated with tier 1 suppliers and 3PL partners Visteon and Harman International for software validation; Received green light from Google for Android Auto deployments with ~ 93.5% rectification of bugs and issues in the initial release; Efficiency improvements by 2% each cycle.\nStrong foundations in CAN, Diagnostics, Android and QNX architecture; Diagnostics tools like CANalyser, CANoe, Garuda 2.0."
  },
  {
    "objectID": "cv/cv.html#software-engineer-intern-oracle-cerner-bangalore-karnataka-india-jan-2022-july-2022",
    "href": "cv/cv.html#software-engineer-intern-oracle-cerner-bangalore-karnataka-india-jan-2022-july-2022",
    "title": "Curriculum Vitae",
    "section": "Software Engineer Intern | Oracle Cerner | Bangalore, Karnataka, India Jan 2022 ‚Äì July 2022",
    "text": "Software Engineer Intern | Oracle Cerner | Bangalore, Karnataka, India Jan 2022 ‚Äì July 2022\n\nCollaborated with cross-functional teams to develop and deploy healthcare software solutions.\nDesigned and implemented secure and scalable software components using Java (spring boot), Python, React, and JavaScript.\nConducted code reviews, debugging, and testing, reducing software defects by 20%.\nEmployed machine learning to cluster suppliers using medical device data, saving the team 10 work hours per week.\nGathered requirements from healthcare professionals and ensured compliance with regulations.\nActively participated in Agile methodologies with practical exposure to machine learning for client data.\nAssignment of beds and nurses to patients in different units done through web application with Spring Security in backend.\n95% average unit test coverage for the mainstream application using Mockito framework and Jest/enzyme framework.\nDevOps: CI/CD of the product upon Jenkins with pipeline staging scripts for automation, integration and deployment.\nEfficiently reduced size of MySQL database by 7% with scripts; improving the product function by ~ 20%."
  },
  {
    "objectID": "cv/cv.html#projects",
    "href": "cv/cv.html#projects",
    "title": "Curriculum Vitae",
    "section": "PROJECTS",
    "text": "PROJECTS"
  },
  {
    "objectID": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib",
    "href": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib",
    "title": "Curriculum Vitae",
    "section": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib",
    "text": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib\n\nPresented Comparative Image analysis between the two data hiding techniques are made on the reconstructed image to conclude which Steganography method achieves better results, Least Significant Bit (LSB) or Discrete Cosine Transform (DST)\nAutomated the generation and evaluation of ~25,000 images, achieving an 87% pose detection accuracy from the model"
  },
  {
    "objectID": "cv/cv.html#idea-repository-api-spring-boot-redux-saga-postman-api-json-git-jenkins-mysql-oracle-database-react-framework",
    "href": "cv/cv.html#idea-repository-api-spring-boot-redux-saga-postman-api-json-git-jenkins-mysql-oracle-database-react-framework",
    "title": "Curriculum Vitae",
    "section": "Idea Repository API | Spring boot, Redux Saga, Postman API, JSON, Git, Jenkins, MySQL, Oracle Database, React Framework",
    "text": "Idea Repository API | Spring boot, Redux Saga, Postman API, JSON, Git, Jenkins, MySQL, Oracle Database, React Framework\n\nAn API for a full-stack web application to establish user security - authorization and authentication at the backend with Redux-based UI focus on frontend.\nPublished as a technical paper - web application improved test user satisfaction ratings by 15%\nEmphasis on using redux-saga middleware instead of Thunk - faster web page response; improved scrolling efficiency by 10%\nGitHub: https://github.com/NishantBharali/projects"
  },
  {
    "objectID": "cv/cv.html#digital-hearing-aid-system-using-matlab-simulation-matlab-gui",
    "href": "cv/cv.html#digital-hearing-aid-system-using-matlab-simulation-matlab-gui",
    "title": "Curriculum Vitae",
    "section": "Digital Hearing Aid System using MATLAB | Simulation, MATLAB (GUI)",
    "text": "Digital Hearing Aid System using MATLAB | Simulation, MATLAB (GUI)\n\nDesigned a digital hearing aid system using MATLAB using Digital Signal Processing. The implementation of this configurable DHA system includes noise reduction filter, frequency shaper function and amplitude compression function.\nThe DHA design is designed to adapt for mild and moderate hearing loss patients since different gain can be set up to map different levels of hearing loss.\nThe code written in MATLAB, loads the input wave signal and takes the sampling frequency and the number of bits of that signal. Then, AWGN (Additive white Gaussian noise) and random noise are added to the signal before they are processed by various MATLAB functions to get an output which is audible to the hearing impaired person."
  },
  {
    "objectID": "cv/cv.html#detection-of-number-plate-and-identification-of-number-using-matlab-platform-simulation-matlab-simulink",
    "href": "cv/cv.html#detection-of-number-plate-and-identification-of-number-using-matlab-platform-simulation-matlab-simulink",
    "title": "Curriculum Vitae",
    "section": "Detection of Number Plate and Identification of Number using MATLAB | Platform Simulation, MATLAB, Simulink",
    "text": "Detection of Number Plate and Identification of Number using MATLAB | Platform Simulation, MATLAB, Simulink\n\nOur project focuses on developing an automated number plate recognition system, streamlining the process of plate detection and information storage. As vehicles enter a secure area, our system automatically captures and stores their number plates, replacing manual data collection for improved accuracy.\nThe project operates on a supervised method, utilizing a reference database for comparison. It comprises three key components: reference creation, plate detection, and alphabet/digit identification."
  },
  {
    "objectID": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib-1",
    "href": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib-1",
    "title": "Curriculum Vitae",
    "section": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib",
    "text": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib\n\nPresented Comparative Image analysis between the two data hiding techniques are made on the reconstructed image to conclude which Steganography method achieves better results, Least Significant Bit (LSB) or Discrete Cosine Transform (DST).\nAutomated the generation and evaluation of 35,000 images, achieving an 87% pose detection accuracy from the model."
  },
  {
    "objectID": "cv/cv.html#adaptive-traffic-light-system-using-8051-microcontroller-8051-micrcontroller-at89c51-leds-7-segment-display-ir-sensors-proteus-simulation",
    "href": "cv/cv.html#adaptive-traffic-light-system-using-8051-microcontroller-8051-micrcontroller-at89c51-leds-7-segment-display-ir-sensors-proteus-simulation",
    "title": "Curriculum Vitae",
    "section": "Adaptive Traffic Light System using 8051 Microcontroller |8051 Micrcontroller (AT89C51), LEDs, 7 Segment Display, IR Sensors, Proteus Simulation",
    "text": "Adaptive Traffic Light System using 8051 Microcontroller |8051 Micrcontroller (AT89C51), LEDs, 7 Segment Display, IR Sensors, Proteus Simulation\n\nDesigned an adaptive traffic light system, which allots different time frames based on the density of traffic to a certain lane.\nUnder current circumstances, traffic lights are set in different directions with a fixed time delay, following a particular cycle which will be conveyed by the LEDs with the time delay in consideration."
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nTechnical Skills: Python, Keras, SciKit-Image (sci-kit learn), Java, MATLAB, SQL, CAN, LIN, HTML, CSS, React, CI/CD, Linux bash, RESTful API, Redux-saga, Spring Framework, Jupyter, Microcontroller 8051, Simulink, SOLIDWORKS, Verilog\nFramework and Tools: OpenCV, Matplotlib (Pillow), Vector CANalyzer, CANoe, Wireshark, Android and QNX architecture, Postman, Git Jenkins, Microservices, ROS, Visual Studio, Gazebo, MySQL\nOther: Agile, Scrum,JIRA, Product& Project Management, MicrosoftOffice Suite, eLMS"
  },
  {
    "objectID": "cv/cv.html#others-and-organizations",
    "href": "cv/cv.html#others-and-organizations",
    "title": "Curriculum Vitae",
    "section": "OTHERS AND ORGANIZATIONS",
    "text": "OTHERS AND ORGANIZATIONS\n\nSAE Autodrive Challenge (Fall 2023) : Participating in the Vehicle Control and testing sub-team under working on Q23-24 cycle learning through training and workshops on topics like Machine Vision, ROS2 and MATLAB GUIs\nUndergraduate Teaching Assistant for the course Digital Logic Design (ECE2003), VIT Vellore (2019-2021) : Secured Silver Rank in IoT - Domain Specialist conforming to National Skills Qualifications Framework Level 8, 2021\nAssistant Web Developer at IEEE IAS, VIT Vellore, 2020\nCore Committee Member, IEEE - Circuits and Systems Society, 2019-2020\nCore Committee Member, IEEE Computer Society, 2019-2022"
  },
  {
    "objectID": "cv/cv.html#publications",
    "href": "cv/cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATION(S)",
    "text": "PUBLICATION(S)\nFull Stack WebDevelopment of Redux-based Applications with Dynamic Microservices(Case Study - IDEA REPOSITORY), 2022-2023\n\nSuccessfully certified and published the technical research paper in a peer-reviewed journal with an acceptable impact factor where the synopsis of the paper was web development strategy to develop applications based on redux using redux-saga middleware instead of its native Thunk middleware for faster web page response and to improve scrolling efficiency by 10%.\nUse of the MySQL server and spring framework was incorporated for dynamic usage of the backend while the saga middleware at the frontend uses the data stored in redux store dynamically and asynchronously to facilitate faster and responsive web page"
  },
  {
    "objectID": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "href": "posts/2019-11-18-Blogdown/2019-11-18-Blogdown.html",
    "title": "Building a Website with Blogdown",
    "section": "",
    "text": "Pablo Barajas, a recent graduate at UC Irvine and friend of Scatter Podcast, led a presentation on how to build a website using R, RStudio, and Blogdown.\nFor anyone wanting to fairly easily build an online portfolio or blog that looks professional, I highly recommend Blogdown. I used this process to build this website, as did Pablo for his personal website, and I can help if you have any questions. Here‚Äôs the process in short: RStudio site build with Blogdown -&gt; Commit to GitHub -&gt; Apply Hugo theme -&gt; Netlify for CI/CD\nThis event was hosted by the Orange County R Users Group (‚ÄúOCRUG‚Äù) and UCI‚Äôs Merage Analytics Club (‚ÄúMAC‚Äù). If you‚Äôre in Orange County and looking to network and learn with local data science practitioners and students, I highly recommend attending OCRUG events, R-Ladies Irvine events, or public MAC events. Great learning opportunities here!\nLinks:\n\nPablo‚Äôs presentation on his Blogdown site\nBlogdown overview from RStudio\nOCRUG‚Äôs homepage\nR-Ladies Irvine chapter\nMerage Analytics Club website\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "href": "posts/2019-12-31-Ubuntu-Update/2019-12-31-Ubuntu-Update.html",
    "title": "Ubuntu Update to 19.10",
    "section": "",
    "text": "Update Ubuntu 19.04 to 19.10 on dual-booted systems. Not sure exactly where things broke down for me, but as of right now, #Ubuntu seems inaccessible. Not too bummed about it as everything is backed up, but if you‚Äôve recently had the same heartburn and found a solution, please send me a message!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "href": "posts/2020-04-05-DnD-Adventures/2020-04-05-DnD-Adventures.html",
    "title": "D&D Adventures + blogdown",
    "section": "",
    "text": "Wizards of the Coast have developed such an incredible tabletop, fantasy, RPG with the 5th edition of D&D‚Ä¶ I wish I would have re-discovered this game years back. For context, I haven‚Äôt touched D&D since 1997!\nIf you know of good resources for newbies and new D&D Dungeon Masters (‚ÄúDMs‚Äù), I‚Äôd love to know about your favorite resources. For data science folks out there, this site was built with R using blogdown, Hugo themes, and deployed for free using GitHub and Netlify.\n\n\n\nD&D dragon looking for trouble\n\n\nReference:  * Javier‚Äôs D&D Adventures\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "href": "posts/2020-12-31-Raspberry-Pi/2020-12-31-Raspberry-Pi.html",
    "title": "Preparing for 2021 Goals with a Raspberry Pi",
    "section": "",
    "text": "Killing a few birds with one stone and getting ready for 2021‚Ä¶ 1) Getting better at Linux / shell commands,  2) Learning Julia (see my Resources tab for some great Julia starter links!),  3) Doing it all on a Raspberry Pi üòú Happy holidays and Happy New Years to all!\nBelow are some pics of my current progress.\n\n\n\nRaspberry Pi 4, Model B, 8GB RAM, & Super Cute Tiny Heat Sinks\n\n\n\n\n\nEnclosing the Pi 4 in a CanaKit case + fan\n\n\n\n\n\nRaspberry Pi OS, a Debian distro for the Pi\n\n\n\n\n\nJupyter Lab server on the Pi set up w/ Python, R, and Julia\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "href": "posts/2021-05-24-dataframe/2021-05-24-dataframe.html",
    "title": "A Brief History of the Dataframe",
    "section": "",
    "text": "This is quite the insightful read on the history of the dataframe. From its origins in the S programming language, to R, to the pandas library for Python (to the tibble for R!)‚Ä¶ I couldn‚Äôt imagine doing my work without this fundamental data structure, but will it really be defined out of existence? See below link for an excellent overview by Devin Petersohn:\n\nSource:\n\nTowards Data Science: Preventing the Death of the Dataframe\npandas for Python\ntibble for R\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#introduction",
    "href": "posts/Anomaly-Outlier Detection/index.html#introduction",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Introduction",
    "text": "Introduction\nIn the modern world of finance, data plays a pivotal role in decision-making processes. Financial datasets often contain critical information about customers, transactions, and more. One crucial aspect of financial data analysis is the detection of anomalies or outliers. Anomalies are data points that significantly deviate from the norm or expected behavior. Detecting these anomalies is essential for fraud detection, risk management, and maintaining data integrity.\nThis report explores the application of anomaly/outlier detection techniques to a financial dataset. The dataset under consideration contains information such as Age, Income, Transaction Amount, Transaction Type, and Location. We aim to identify unusual patterns or data points within this dataset that may require further investigation.\nAdvanced Analysis\nI will now conduct an advanced analysis of this dataset, focusing on the following aspects:\nData Exploration: Understanding the basic statistics of the dataset. Visualization: Plotting the data to observe any unusual patterns. Anomaly Detection Techniques: Implementing various methods to detect outliers, such as statistical methods, clustering-based methods, and machine learning models. For this analysis, I will be using Python libraries like Pandas, NumPy, Matplotlib, Seaborn, and Scikit-Learn. Let‚Äôs start with the data exploration and visualization.\nAnalysis Overview\nData Exploration\nThe descriptive statistics of the dataset provide a basic understanding of its features:\nA. Age: Ranges from 18 to 69, with an average of around 43 years. B. Income: The average income is approximately $51,518, with a wide range from around $988 to $181,196, suggesting significant variance. C. Transaction Amount: On average, transactions are around $200, but there are values as high as $1,999, which might indicate potential outliers.\nVisualization Insights\nA. Histograms: These show the distribution of numerical features. While age appears fairly uniformly distributed, income and transaction amounts show right-skewed distributions. B. Boxplots: The boxplots highlight potential outliers, especially in the income and transaction amount data. PCA Scatter Plot: After applying PCA for dimensionality reduction, we get a 2D visualization of the scaled numerical data. This plot can help us identify clusters and potential outliers visually.\nNext Steps for Anomaly Detection\nBased on these visualizations and statistics, the next step is to apply anomaly detection techniques. I will use the Isolation Forest algorithm, a popular method for outlier detection, especially effective with high-dimensional datasets. This method isolates anomalies instead of profiling normal data points. Anomalies are few and different, hence easier to isolate.\nData Exploration and Analysis\nI applied the Isolation Forest algorithm to our scaled numerical data and identify the anomalies. Let‚Äôs proceed with this analysis.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Loading the dataset\ndf = pd.read_csv('anomaly_detection_dataset.csv')\n\n# Data Exploration: Descriptive statistics\ndescriptive_stats = df.describe()\n# Display basic statistics\n\n# Visualization\n# Histograms for numerical data\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndf[['Age', 'Income', 'Transaction Amount']].hist(bins=15, ax=axes, color='skyblue')\nplt.suptitle('Histograms of Numerical Features')\n\n# Boxplot for numerical data to check for outliers\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\nsns.boxplot(data=df[['Age', 'Income', 'Transaction Amount']], ax=axes[0])\nsns.boxplot(data=df[['Income']], ax=axes[1])\nsns.boxplot(data=df[['Transaction Amount']], ax=axes[2])\nplt.suptitle('Boxplots of Numerical Features')\n\n# Preparing data for anomaly detection\n# Standardizing the numerical data\nscaler = StandardScaler()\nscaled_numerical_data = scaler.fit_transform(df[['Age', 'Income', 'Transaction Amount']])\n\n# Applying PCA for dimensionality reduction (2D visualization)\npca = PCA(n_components=2)\npca_results = pca.fit_transform(scaled_numerical_data)\npca_df = pd.DataFrame(data=pca_results, columns=['PC1', 'PC2'])\n\n# Scatter plot of PCA results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', data=pca_df)\nplt.title('PCA of Scaled Numerical Data')\n\nplt.show()\n\ndescriptive_stats, pca_df.head()\n\n\n\n\n\n\n\n\n\n\n(               Age         Income  Transaction Amount\n count  1000.000000    1000.000000         1000.000000\n mean     43.267000   51518.424999          200.855857\n std      15.242311   18506.474035          197.923861\n min      18.000000     988.660890            0.381117\n 25%      30.000000   39745.904804           79.905356\n 50%      43.000000   50483.467494          162.361081\n 75%      56.000000   60698.045016          264.145550\n max      69.000000  181196.443031         1999.137390,\n         PC1       PC2\n 0 -0.301194 -1.080589\n 1 -0.282737 -0.824039\n 2 -0.474600  2.067434\n 3  0.344241  0.305833\n 4 -0.009123  0.894454)\n\n\nData Visualization\nNext, we visualize the data to identify any obvious outliers or patterns.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histograms for numerical features\ndf.hist(bins=15, figsize=(15, 6))\nplt.show()\n\n# Box plots for numerical features to identify outliers\ndf.plot(kind='box', subplots=True, layout=(2,3), figsize=(15, 8))\nplt.show()\n\n\n\n\n\n\n\nAnalysis Results: Anomaly Detection with Isolation Forest\nAnomaly Detection\nThe Isolation Forest algorithm was applied to the scaled numerical data, and it identified 50 anomalies in our dataset. This is consistent with the initial contamination rate set to 5% of the data.\nVisualization of Detected Anomalies\nThe scatter plot based on the PCA results with anomalies highlighted shows:\nA. Normal data points in blue. B. Anomalies marked in red.\nThese anomalies represent unusual patterns in terms of age, income, and transaction amounts, as identified by the Isolation Forest algorithm.\nInterpretation\nThe visualization clearly shows that the anomalies are distinct from the bulk of the data, signifying their outlier status. These could represent unusual financial transactions or demographic anomalies that would be of interest in real-world scenarios like fraud detection or targeted marketing.\nAnomaly Detection Using Isolation Forest\nWe apply the Isolation Forest algorithm to detect anomalies in the dataset. This method is effective for high-dimensional datasets and does not require prior knowledge of the number of anomalies.\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))\n\n# Applying Isolation Forest\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=0)\npredictions = iso_forest.fit_predict(scaled_data)\n\n# Add a column for anomaly (1 for normal, -1 for anomaly)\ndf['anomaly'] = predictions\n\n# Count the number of anomalies\nanomaly_count = df['anomaly'].value_counts()\nprint(anomaly_count)\n\nanomaly\n 1    835\n-1    165\nName: count, dtype: int64\n\n\nVisualizing the Anomalies\nWe can visualize the anomalies in the context of two principal components.\n\nfrom sklearn.decomposition import PCA\n\n# PCA for dimensionality reduction for visualization\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_data)\n\ndf['pca1'] = pca_result[:, 0]\ndf['pca2'] = pca_result[:, 1]\n\n# Scatter plot of the PCA results colored by anomaly\nsns.scatterplot(x='pca1', y='pca2', hue='anomaly', data=df, palette={1: 'blue', -1: 'red'})\nplt.title('Anomalies in the PCA Plane')\nplt.show()\n\n\n\n\nK-Means Clustering for Anomaly Detection\nK-Means can be used for anomaly detection by identifying small clusters as anomalies.\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# K-Means Clustering with explicit n_init parameter\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=0).fit(scaled_data)\ndf['cluster'] = kmeans.labels_\n\n# Detecting outliers as the points farthest from the centroids\ndistances = kmeans.transform(scaled_data)\ndf['distance_to_centroid'] = np.min(distances, axis=1)\noutlier_threshold = np.percentile(df['distance_to_centroid'], 95)\ndf['outlier'] = df['distance_to_centroid'] &gt; outlier_threshold\n\nK-Means Clustering Visualization\nAfter running the K-Means algorithm, we can plot the data points and color them by their cluster. Points classified as outliers will be highlighted.\n\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette='Set1')\nsns.scatterplot(x=df[df['outlier']]['pca1'], y=df[df['outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('K-Means Clustering')\nplt.legend()\nplt.show()\n\n\n\n\nDBSCAN for Anomaly Detection\nDBSCAN is effective in identifying regions of high density and isolating outliers as points in low-density areas.\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN Clustering\ndbscan = DBSCAN(eps=0.5, min_samples=10).fit(scaled_data)\ndf['dbscan_labels'] = dbscan.labels_\n\n# Anomalies are points with label -1\ndf['dbscan_outlier'] = df['dbscan_labels'] == -1\n\nDBSCAN Visualization\nFor DBSCAN, we will plot the data points and color them based on their cluster, highlighting the anomalies.\n\n# DBSCAN Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='dbscan_labels', data=df, palette='Set2')\nsns.scatterplot(x=df[df['dbscan_outlier']]['pca1'], y=df[df['dbscan_outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('DBSCAN Clustering')\nplt.legend()\nplt.show()\n\n\n\n\nLocal Outlier Factor (LOF)\nLOF computes a score reflecting the degree of abnormality of the data, considering local density.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# LOF for anomaly detection\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\nlof_labels = lof.fit_predict(scaled_data)\n\ndf['lof_outlier'] = lof_labels == -1\n\nLOF Visualization\nLocal Outlier Factor results can be visualized by highlighting the points that are identified as outliers.\n\n# LOF Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='lof_outlier', data=df)\nplt.title('Local Outlier Factor')\nplt.show()\n\n\n\n\nT-SNE for Visualization of High-Dimensional Data\nT-SNE is effective for visualizing high-dimensional data and its anomalies in a lower-dimensional space.\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make sure to replace 'scaled_data' with your actual scaled data variable name\ntsne = TSNE(n_components=2, random_state=0)\ntsne_results = tsne.fit_transform(scaled_data)\n\n# Adding T-SNE results to your DataFrame\ndf['tsne1'] = tsne_results[:, 0]\ndf['tsne2'] = tsne_results[:, 1]\n\n# Use 'dbscan_labels' or another column of your choice for coloring the points\ncolumn_for_coloring = 'dbscan_labels'  # Replace with your chosen column\n\n# T-SNE Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='tsne1', y='tsne2', hue=column_for_coloring, data=df, palette='Set2', legend=\"full\")\nplt.title('T-SNE Visualization')\nplt.show()\n\n\n\n\nT-SNE results can be plotted to show how the data is distributed in the reduced-dimensional space, highlighting the anomalies detected.\nThese techniques represent a deeper dive into anomaly detection, leveraging different approaches from clustering, neural networks, and neighborhood-based methods. Each method has its strengths and can be combined or compared for a comprehensive anomaly detection strategy.\nFinal check for Anomaly detection\n\n# Applying Isolation Forest for anomaly detection\niso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=0)\nanomalies = iso_forest.fit_predict(scaled_numerical_data)\n\n# Adding the anomaly predictions to the PCA DataFrame\npca_df['Anomaly'] = anomalies\n\n# Visualizing the identified anomalies on the PCA plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', hue='Anomaly', data=pca_df, palette={1: 'blue', -1: 'red'})\nplt.title('PCA of Scaled Numerical Data with Anomalies Highlighted')\nplt.legend(title='Anomaly', loc='upper right', labels=['Normal', 'Anomaly'])\n\nplt.show()\n\n# Counting the number of detected anomalies\nnum_anomalies_detected = (anomalies == -1).sum()\n\nnum_anomalies_detected, pca_df.head()\n\n\n\n\n(50,\n         PC1       PC2  Anomaly\n 0 -0.301194 -1.080589        1\n 1 -0.282737 -0.824039        1\n 2 -0.474600  2.067434        1\n 3  0.344241  0.305833        1\n 4 -0.009123  0.894454        1)\n\n\nConclusion\nIn conclusion, the analysis of anomalies and outliers in financial datasets is of paramount importance for safeguarding financial systems and ensuring data quality. In this report, we delved into the application of various techniques to detect anomalies in a financial dataset containing Age, Income, Transaction Amount, Transaction Type, and Location. The results of this analysis can guide decision-makers in identifying potential issues, such as fraudulent transactions or data entry errors.\nBy leveraging advanced analytics and machine learning algorithms, financial institutions can proactively mitigate risks, enhance customer trust, and streamline their operations. The continuous monitoring and refinement of anomaly detection methods are critical in the ever-evolving landscape of finance.\nAs financial data continues to grow in complexity and volume, the ability to detect anomalies accurately becomes increasingly crucial. It is essential for organizations to stay vigilant and adapt their anomaly detection strategies to stay ahead of emerging threats and challenges in the financial sector."
  },
  {
    "objectID": "posts/Clustering/index.html#employing-sophisticated-clustering-techniques-with-retail-customer-data-and-comparing-further-with-prediction-models",
    "href": "posts/Clustering/index.html#employing-sophisticated-clustering-techniques-with-retail-customer-data-and-comparing-further-with-prediction-models",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Employing sophisticated clustering techniques with retail customer data and comparing further with prediction models",
    "text": "Employing sophisticated clustering techniques with retail customer data and comparing further with prediction models\nClustering is a fundamental technique in machine learning that involves grouping data points so that the objects in the same group (or cluster) are more similar to each other than to those in other groups. It‚Äôs a form of unsupervised learning, as the groups are not predefined but rather determined by the algorithm itself. This approach is particularly useful in understanding the structure within data, identifying patterns, and making strategic decisions.\nIn this blog, we will explore how to apply clustering techniques to a customer dataset. Our dataset contains customer information with attributes like Customer ID, Age, Annual Income, and Spending Score. The goal is to segment customers into distinct groups based on these features, which can help in tailoring marketing strategies, understanding customer behavior, and improving customer service.\nIntroduction\nData Loading and Basic Visualization\n\n# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncustomer_df = pd.read_csv('clusteringCustomerData.csv')\nprint(customer_df.head())\n\n   CustomerID  Age  Annual_Income  Spending_Score\n0           1   62             75              81\n1           2   65             76              23\n2           3   18             98              80\n3           4   21             48              90\n4           5   21             47               9\n\n\nExploring Data Analysis\n\n#Visualize the distributions and relationships between features.\nsns.pairplot(customer_df.drop('CustomerID', axis=1))\nplt.show()\n\n\n\n\nData Preprocessing\n\n#Encoding categorical data and scaling features.\n#Standardize the data.\n# Selecting features to be scaled\nfeatures = customer_df[['Age', 'Annual_Income', 'Spending_Score']]\n\n# Standardizing the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\n# Converting scaled features back to a DataFrame\nscaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n\n# Displaying the first few rows of the scaled data\nprint(scaled_features_df.head())\n\n        Age  Annual_Income  Spending_Score\n0  1.237684       0.746278        0.901945\n1  1.428146       0.788416       -1.020966\n2 -1.555756       1.715469        0.868791\n3 -1.365294      -0.391469        1.200327\n4 -1.365294      -0.433608       -1.485117\n\n\nExploratory Data Analysis (EDA)\nBeyond basic visualizations, we‚Äôll use EDA to understand the data distributions and potential relationships.\n\n# Visualizing the distribution of features\nplt.figure(figsize=(12, 4))\nfor i, col in enumerate(['Age', 'Annual_Income', 'Spending_Score']):\n    plt.subplot(1, 3, i+1)\n    sns.histplot(customer_df[col], kde=True)\n    plt.title(f'Distribution of {col}')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#advanced-clustering-with-multiple-techniques",
    "href": "posts/Clustering/index.html#advanced-clustering-with-multiple-techniques",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Advanced Clustering with Multiple Techniques",
    "text": "Advanced Clustering with Multiple Techniques\nWe‚Äôll explore different clustering algorithms beyond K-Means, such as Hierarchical Clustering and DBSCAN, to understand how they segment the data differently."
  },
  {
    "objectID": "posts/Clustering/index.html#k-means-clustering",
    "href": "posts/Clustering/index.html#k-means-clustering",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "1. K-Means Clustering",
    "text": "1. K-Means Clustering\n\n#Using the Elbow Method to determine the optimal number of clusters.\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n    kmeans.fit(scaled_features)\n    wcss.append(kmeans.inertia_)\n    customer_df['KMeans_Cluster'] = kmeans.labels_\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\nApplication of K-Means\n\n#Applying K-Means and visualizing the clusters.\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=42)\ncustomer_df['Cluster'] = kmeans.fit_predict(scaled_features)\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='Cluster', data=customer_df, palette='viridis')\nplt.title('Customer Segments')\nplt.show()\n\n\n\n\nK-Means Visualization\n\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='KMeans_Cluster', palette='viridis')\nplt.suptitle('K-Means Clustering', y=1.02)\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#dbscan-clustering",
    "href": "posts/Clustering/index.html#dbscan-clustering",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "2. DBSCAN clustering",
    "text": "2. DBSCAN clustering\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5).fit(scaled_features_df)\ncustomer_df['DBSCAN_Cluster'] = dbscan.labels_\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) Visualization\n\n# Visualizing DBSCAN Clusters\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='DBSCAN_Cluster', data=customer_df, palette='viridis')\nplt.title('DBSCAN Clustering')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#hierarchical-cluster-visualization",
    "href": "posts/Clustering/index.html#hierarchical-cluster-visualization",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "3. Hierarchical Cluster Visualization",
    "text": "3. Hierarchical Cluster Visualization\nVisualize the clusters formed by each algorithm in multiple dimensions to gain deeper insights.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering  # Import AgglomerativeClustering\n\n# Performing Hierarchical Clustering\nlinked = linkage(scaled_features_df, method='ward')\n\n# Plotting the Dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n# Perform hierarchical clustering and add the cluster labels to customer_df\nclustering = AgglomerativeClustering(n_clusters=3)  # Specify the number of clusters\ncustomer_df['Hierarchical_Cluster'] = clustering.fit_predict(scaled_features_df)\n\n# Hierarchical Clustering Visualization\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='Hierarchical_Cluster', palette='viridis')\nplt.suptitle('Hierarchical Clustering', y=1.02)\nplt.show()\n\n\n\n\n\n\n\nHierarchical Clustering Cut\n\nfrom scipy.cluster.hierarchy import fcluster\n\n# Cutting the Dendrogram to form clusters\ncustomer_df['Hierarchical_Cluster'] = fcluster(linked, t=3, criterion='maxclust')\n\nsns.scatterplot(data=customer_df, x='Annual_Income', y='Spending_Score', hue='Hierarchical_Cluster', palette='viridis')\nplt.title('Hierarchical Clustering')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-analysis-for-k-means-and-hierarchical-clustering",
    "href": "posts/Clustering/index.html#silhouette-analysis-for-k-means-and-hierarchical-clustering",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Silhouette Analysis for K-means and Hierarchical Clustering",
    "text": "Silhouette Analysis for K-means and Hierarchical Clustering\n1. K-Means Clustering\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\n\n# Calculate silhouette scores for different cluster numbers\nsilhouette_scores = []\nfor n_clusters in range(2, 11):\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    cluster_labels = kmeans.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plot silhouette scores\nplt.plot(range(2, 11), silhouette_scores, marker='o')\nplt.title('Silhouette Analysis for K-Means')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\n\n2. Hierarchical Clustering\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Calculate silhouette scores for different cluster numbers in hierarchical clustering\nsilhouette_scores_hierarchical = []\nfor n_clusters in range(2, 11):\n    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n    cluster_labels = hierarchical.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores_hierarchical.append(silhouette_avg)\n\n# Plot silhouette scores for hierarchical clustering\nplt.plot(range(2, 11), silhouette_scores_hierarchical, marker='o')\nplt.title('Silhouette Analysis for Hierarchical Clustering')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#conclusion",
    "href": "posts/Clustering/index.html#conclusion",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Conclusion",
    "text": "Conclusion\n#In this in-depth analysis, we have explored different clustering techniques and visualized their results to segment customers in a comprehensive manner. Each method offers unique insights: K-Means provides clear segmentations, Hierarchical Clustering helps us understand the data structure, and DBSCAN identifies core and outlier points effectively.\nBy comparing these methods, we can choose the one that best suits our specific needs for customer segmentation. This advanced clustering analysis can guide strategic decisions, improve customer engagement, and enhance targeting in marketing campaigns."
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#utilizing-linear-and-non-linear-regression-methodologies-for-the-purpose-of-analyzing-trends-within-the-housing-market",
    "href": "posts/Linear and Non Linear Regression/index.html#utilizing-linear-and-non-linear-regression-methodologies-for-the-purpose-of-analyzing-trends-within-the-housing-market",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Utilizing linear and non-linear regression methodologies for the purpose of analyzing trends within the housing market",
    "text": "Utilizing linear and non-linear regression methodologies for the purpose of analyzing trends within the housing market\nIntroduction\nIn machine learning, linear and nonlinear regression are fundamental techniques used to model relationships between variables, such as predicting housing prices based on various features in a dataset.\nLinear Regression is a straightforward method that assumes a linear relationship between the input features (e.g., square footage, number of bedrooms) and the target variable (housing price). It aims to find the best-fit line that minimizes the difference between predicted and actual values. Linear regression is interpretable and works well when the relationship is approximately linear.\nNonlinear Regression, on the other hand, allows for more complex relationships. It can capture curves, bends, and nonlinear patterns in the data. This is particularly useful when housing prices may depend on interactions between features or exhibit nonlinear behavior.\nThe housing dataset you are using provides a rich source of information to apply both linear and nonlinear regression techniques. By utilizing these methods, you can build predictive models that estimate housing prices accurately, taking into account the specific relationships between features and target variables, whether they are linear or nonlinear in nature. These models can guide real estate decisions, investment strategies, and market analyses more effectively, ultimately benefiting both buyers and sellers in the housing market.\nWe will start by loading the dataset and performing basic preprocessing, including encoding categorical variables and feature scaling.\nExploring the Dataset and Data Preprocessing\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\n\n\n# Load the dataset\nhousing_df = pd.read_csv('modified_housing_data.csv')\n\n# Handling missing values if any\nimputer = SimpleImputer(strategy='mean')\nhousing_df[['Size', 'GardenArea']] = imputer.fit_transform(housing_df[['Size', 'GardenArea']])\n\n# One-hot encoding and Scaling\ncategorical_features = ['Neighborhood']\nnumerical_features = ['Size', 'Bedrooms', 'Bathrooms', 'Age', 'GarageSize', 'GardenArea']\n\n# Create transformers\none_hot = OneHotEncoder()\nscaler = StandardScaler()\n\n# Column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', scaler, numerical_features),\n        ('cat', one_hot, categorical_features)\n    ])\n\n# Preprocessing pipeline\nprep_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n# Splitting the data\nX = housing_df.drop(['HouseID', 'Price'], axis=1)\ny = housing_df['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_prep = prep_pipeline.fit_transform(X_train)\nX_test_prep = prep_pipeline.transform(X_test)\n\nVisualizing Market Trend\n\n#Visualizing the distributions and relationships\nsns.pairplot(housing_df)\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-linear-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-linear-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Linear Regression Analysis",
    "text": "Advanced Linear Regression Analysis\nFor the linear regression model, we‚Äôll include feature importance analysis and cross-validation.\n\n# Linear Regression Model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_prep, y_train)\n\n# Cross-Validation\ncv_scores = cross_val_score(linear_model, X_train_prep, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"CV MSE for Linear Regression:\", -np.mean(cv_scores))\n\n# Predictions and Evaluation\ny_pred_linear = linear_model.predict(X_test_prep)\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression Test MSE:\", mse_linear)\nprint(\"Linear Regression Test R2:\", r2_linear)\n\nCV MSE for Linear Regression: 2618960221.1247973\nLinear Regression Test MSE: 2520672652.9396286\nLinear Regression Test R2: 0.9111757572070075"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Non-Linear Regression Analysis",
    "text": "Advanced Non-Linear Regression Analysis\nWe‚Äôll apply a more complex non-linear model, such as a Random Forest Regressor, and perform hyperparameter tuning using GridSearchCV.\n\n# Non-Linear Model - Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=42)\n\n# Hyperparameter Grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_features': ['sqrt', 'log2', None],  # Removed 'auto' and added None\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid Search\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(X_train_prep, y_train)\n\n# Best Model\nbest_rf_model = grid_search.best_estimator_\n\n# Predictions and Evaluation\ny_pred_rf = best_rf_model.predict(X_test_prep)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\nprint(\"Random Forest Test MSE:\", mse_rf)\nprint(\"Random Forest Test R2:\", r2_rf)\n\nFitting 3 folds for each of 324 candidates, totalling 972 fits\nRandom Forest Test MSE: 3815896212.7555285\nRandom Forest Test R2: 0.865534268688408"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Regression Analysis",
    "text": "Advanced Regression Analysis\n1. Exploring Feature Interactions\nFeature interactions can reveal complex relationships that might not be captured by individual features alone.\n\n# Adding interaction terms\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train_poly = poly.fit_transform(X_train_prep)\nX_test_poly = poly.transform(X_test_prep)\n\n# Re-training Linear Regression with Interaction Terms\nlinear_model_interact = LinearRegression()\nlinear_model_interact.fit(X_train_poly, y_train)\n\n# Evaluating the model with interaction terms\ny_pred_interact = linear_model_interact.predict(X_test_poly)\nmse_interact = mean_squared_error(y_test, y_pred_interact)\nprint(\"MSE with Interaction Terms:\", mse_interact)\n\nMSE with Interaction Terms: 2983950454.9349637\n\n\n2. Model Diagnostics for Linear Regression\nChecking assumptions and diagnostics of linear regression to ensure the validity of the model.\n\n# Model Diagnostics\nX_train_sm = sm.add_constant(X_train_prep)  # Adding a constant\nmodel = sm.OLS(y_train, X_train_sm).fit()\nprint(model.summary())\n\n# Residuals plot\nplt.scatter(model.predict(X_train_sm), model.resid)\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Predicted')\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.917\nModel:                            OLS   Adj. R-squared:                  0.914\nMethod:                 Least Squares   F-statistic:                     283.3\nDate:                Sun, 10 Dec 2023   Prob (F-statistic):          3.91e-119\nTime:                        03:45:05   Log-Likelihood:                -2932.4\nNo. Observations:                 240   AIC:                             5885.\nDf Residuals:                     230   BIC:                             5920.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.798e+05   2602.372    184.377      0.000    4.75e+05    4.85e+05\nx1           1.56e+05   3262.250     47.819      0.000     1.5e+05    1.62e+05\nx2          5.035e+04   3267.184     15.410      0.000    4.39e+04    5.68e+04\nx3          2.194e+04   3304.761      6.640      0.000    1.54e+04    2.85e+04\nx4         -1.011e+04   3260.647     -3.100      0.002   -1.65e+04   -3684.133\nx5         -2624.2257   3281.530     -0.800      0.425   -9089.930    3841.478\nx6           -41.7240   3263.210     -0.013      0.990   -6471.330    6387.882\nx7          1.215e+05   5467.126     22.218      0.000    1.11e+05    1.32e+05\nx8           1.26e+05   5609.374     22.456      0.000    1.15e+05    1.37e+05\nx9          1.184e+05   6164.082     19.216      0.000    1.06e+05    1.31e+05\nx10         1.139e+05   5567.918     20.463      0.000    1.03e+05    1.25e+05\n==============================================================================\nOmnibus:                        0.912   Durbin-Watson:                   2.215\nProb(Omnibus):                  0.634   Jarque-Bera (JB):                0.697\nSkew:                          -0.123   Prob(JB):                        0.706\nKurtosis:                       3.097   Cond. No.                     6.20e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 7.83e-30. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\n\n3. Learning Curves\nUnderstanding how model performance changes as the training set size increases.\n\n# Learning curve function\ndef plot_learning_curve(estimator, X, y, title):\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    plt.grid()\n    plt.show()\n\n# Plotting learning curve for Linear Regression\nplot_learning_curve(linear_model, X_train_prep, y_train, \"Linear Regression Learning Curve\")\n\n\n\n\n4. Ensemble Methods\nCombining multiple regression models to improve predictive performance.\n\n# Gradient Boosting Regressor\ngb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_gb = gb_reg.predict(X_test_prep)\nmse_gb = mean_squared_error(y_test, y_pred_gb)\nprint(\"Gradient Boosting Regressor MSE:\", mse_gb)\n\nGradient Boosting Regressor MSE: 3673304675.984175"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#deeper-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#deeper-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Deeper Regression Analysis",
    "text": "Deeper Regression Analysis\nFeature Importance-Based Selection\nFirst, let‚Äôs use a model to identify the most important features and then retrain our models using only these features.\n\n# Feature Importance with Random Forest\nrf_for_importance = RandomForestRegressor()\nrf_for_importance.fit(X_train_prep, y_train)\n\n# Get feature importances and corresponding feature names\nimportances = rf_for_importance.feature_importances_\nfeature_names = prep_pipeline.get_feature_names_out()\n\n# Creating a DataFrame for visualization\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n\n# Adjusting the threshold for feature selection\ntop_features = importance_df[importance_df['Importance'].cumsum() &lt;= 0.90]['Feature']\n\n# Ensure that top_features is not empty\nif len(top_features) == 0:\n    raise ValueError(\"No features selected. Consider loosening the feature selection criterion.\")\n\nX_train_top = X_train_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\nX_test_top = X_test_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\n\n# Re-train models with top features\nlinear_model_top = LinearRegression()\nlinear_model_top.fit(X_train_top, y_train)\n\n# Evaluation\ny_pred_top = linear_model_top.predict(X_test_top)\nmse_top = mean_squared_error(y_test, y_pred_top)\nprint(\"Top Features Linear Regression MSE:\", mse_top)\n\nTop Features Linear Regression MSE: 5967275811.385474"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-models",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-models",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Non-Linear Models",
    "text": "Advanced Non-Linear Models\nIncorporating more complex non-linear models such as Support Vector Regression and Neural Networks.\n\n# Support Vector Regression\nsvr = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_svr = svr.predict(X_test_prep)\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nprint(\"Support Vector Regression MSE:\", mse_svr)\n\nnn_reg = MLPRegressor(hidden_layer_sizes=(100, 50), \n                      max_iter=5000, \n                      learning_rate_init=0.001, \n                      solver='adam', \n                      early_stopping=True, \n                      n_iter_no_change=10,\n                      random_state=42)\nnn_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_nn = nn_reg.predict(X_test_prep)\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nprint(\"Neural Network Regression MSE:\", mse_nn)\n\nSupport Vector Regression MSE: 28406556463.671677\nNeural Network Regression MSE: 397201687873.44415\n\n\nBuilding and Evaluating the Linear Regression Model\nA. Final Model Training\nWe‚Äôll train the final models using the best parameters found from previous steps.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Linear Regression\nlinear_model_final = LinearRegression()\nlinear_model_final.fit(X_train_prep, y_train)\n\n# Neural Network\nnn_model_final = MLPRegressor(hidden_layer_sizes=(100, 50), \n                              max_iter=5000, \n                              learning_rate_init=0.001, \n                              solver='adam', \n                              early_stopping=True, \n                              n_iter_no_change=10,\n                              random_state=42)\nnn_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_linear = linear_model_final.predict(X_test_prep)\ny_pred_nn = nn_model_final.predict(X_test_prep)\n\nB. Evaluation Metrics\nCalculating and printing evaluation metrics for both models. We‚Äôll train the final models using the best parameters found from previous steps.\n\n# Evaluation for Linear Regression\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression - MSE:\", mse_linear, \"R2:\", r2_linear)\n\n# Evaluation for Neural Network\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\nprint(\"Neural Network - MSE:\", mse_nn, \"R2:\", r2_nn)\n\nLinear Regression - MSE: 2520672652.9396286 R2: 0.9111757572070075\nNeural Network - MSE: 397201687873.44415 R2: -12.996715964015438\n\n\nC. Plotting Model Performance\nVisualizing the performance of the models using scatter plots and residual plots.\nI. Scatter Plot for Predictions.\n\nplt.figure(figsize=(12, 6))\n\n# Scatter plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Scatter plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nResidual Plot\n\n\nplt.figure(figsize=(12, 6))\n\n# Residual plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Residual plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nBuilding and Evaluating Non-Linear Model (Support Vector Regression)\nA. Training the Support Vector Regressor\n\nfrom sklearn.svm import SVR\n\n# Support Vector Regression\nsvr_model_final = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_svr = svr_model_final.predict(X_test_prep)\n\nB. Evaluation Metrics for SVR\n\n# Evaluation for Support Vector Regression\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\nprint(\"Support Vector Regression - MSE:\", mse_svr, \"R2:\", r2_svr)\n\nSupport Vector Regression - MSE: 28406556463.671677 R2: -0.0009990251211158263\n\n\nC. Plotting Comparisons\nVisualizing the performance of the Linear Regression, Neural Network, and Support Vector Regression models.\n\nplt.figure(figsize=(18, 6))\n\n# Linear Regression\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Neural Network\nplt.subplot(1, 3, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\n# Support Vector Regression\nplt.subplot(1, 3, 3)\nplt.scatter(y_test, y_pred_svr, alpha=0.5, color='green')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Support Vector Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nD. Residual Plot for All Models\n\nplt.figure(figsize=(18, 6))\n\n# Linear Regression Residuals\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Neural Network Residuals\nplt.subplot(1, 3, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\n# SVR Residuals\nplt.subplot(1, 3, 3)\nplt.scatter(y_pred_svr, y_test - y_pred_svr, alpha=0.5, color='green')\nplt.hlines(y=0, xmin=y_pred_svr.min(), xmax=y_pred_svr.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Support Vector Regression Residuals')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#conclusion",
    "href": "posts/Linear and Non Linear Regression/index.html#conclusion",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the fields of random processes and probability theory, as well as linear and nonlinear regression, are vital components of machine learning when applied to diverse datasets such as weather and housing data. These foundational concepts empower us to model and make predictions in the face of inherent uncertainty, allowing for more accurate forecasts, informed decision-making, and improved insights.\nFor weather data analysis, random processes and probability theory enable us to understand and quantify the stochastic nature of weather patterns. Leveraging machine learning techniques on this data helps us provide accurate forecasts and anticipate extreme events, benefiting numerous sectors that rely on weather information.\nIn the case of housing data analysis, linear and nonlinear regression techniques enable us to model complex relationships between housing features and prices. Whether it‚Äôs linear relationships for straightforward cases or nonlinear models to capture intricate patterns, these tools empower us to make more informed decisions in real estate, investments, and market analysis.\nIn both domains, machine learning applied to these fundamental concepts provides us with the means to extract valuable insights and make data-driven decisions, ultimately enhancing our understanding and predictive capabilities, and offering practical solutions that can improve the quality of life and the efficiency of various industries."
  },
  {
    "objectID": "posts/Random Processes/index.html",
    "href": "posts/Random Processes/index.html",
    "title": "Probability Theory and Random Processes",
    "section": "",
    "text": "In this blog post I will discuss a few examples of probability in machine learning. If you are new to probability, I recommend one of great textbooks that cover the topic and are available for free online, such as Think Bayes by Allen Downey and Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu.\nClassification algorithms algorithms can estimate \\(n \\times k\\) class membership probabilities for each dataset, where n is the number of data points in the dataset and k is the number of classes in the training dataset. Similarly, the Gaussian Mixtures clustering algorithm can generate \\(n \\times k\\) cluster label probabilities.\nBesides a data point and the Gaussian Mixtures models can estimate cluster membership probability. point , especially Logistic Regression and Naive Bayes. Every classification algorithm can estimate probabilities of belonging to each class.\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#gaining-insights-on-weather-data-with-data-science-and-machine-learning",
    "href": "posts/Random Processes/index.html#gaining-insights-on-weather-data-with-data-science-and-machine-learning",
    "title": "Probability Theory and Random Processes",
    "section": "",
    "text": "In this blog post I will discuss a few examples of probability in machine learning. If you are new to probability, I recommend one of great textbooks that cover the topic and are available for free online, such as Think Bayes by Allen Downey and Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu.\nClassification algorithms algorithms can estimate \\(n \\times k\\) class membership probabilities for each dataset, where n is the number of data points in the dataset and k is the number of classes in the training dataset. Similarly, the Gaussian Mixtures clustering algorithm can generate \\(n \\times k\\) cluster label probabilities.\nBesides a data point and the Gaussian Mixtures models can estimate cluster membership probability. point , especially Logistic Regression and Naive Bayes. Every classification algorithm can estimate probabilities of belonging to each class.\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#understanding-weather-forecasting-with-the-help-of-probability-theory-and-random-processes",
    "href": "posts/Random Processes/index.html#understanding-weather-forecasting-with-the-help-of-probability-theory-and-random-processes",
    "title": "Probability Theory and Random Processes",
    "section": "Understanding Weather Forecasting with the help of Probability theory and Random Processes",
    "text": "Understanding Weather Forecasting with the help of Probability theory and Random Processes\nIntroduction\nMachine learning plays a pivotal role in understanding and predicting various natural phenomena, and weather forecasting is no exception. To harness the power of machine learning for weather data analysis, it is essential to have a solid foundation in random processes and probability theory. These fundamental concepts are the building blocks that enable us to model the inherent uncertainty and variability present in weather data.\nWeather data, such as temperature, humidity, wind speed, and precipitation, exhibit random behavior due to the complex interplay of atmospheric processes. Random processes are mathematical models used to describe the evolution of these variables over time. These processes capture the idea that weather conditions are not deterministic but rather stochastic, influenced by a multitude of factors, including geographical location, time of year, and local phenomena.\nProbability theory, on the other hand, provides the framework to quantify and reason about uncertainty in weather data. It allows us to assign probabilities to different weather outcomes and make informed predictions based on observed data. For example, we can calculate the probability of rain on a given day or estimate the likelihood of extreme weather events, such as hurricanes or heatwaves, occurring in a specific region.\nMachine learning techniques, such as regression, classification, and time series analysis, heavily rely on probabilistic and random process models to extract meaningful insights from weather data. By incorporating these techniques, we can build predictive models that not only provide accurate weather forecasts but also account for uncertainty, enabling better decision-making in various applications like agriculture, transportation, and disaster management.\nIn this context, the weather dataset you are using serves as a valuable source of information for exploring and applying these concepts. By understanding random processes and probability theory, you can leverage machine learning to unlock the potential hidden within weather data, improving the accuracy and reliability of weather forecasts and facilitating data-driven decision-making in various sectors that rely on weather information.\nData Loading and Basic Visualization\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import expon\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load the dataset\nweather_df = pd.read_csv('weather_data.csv')\nweather_df['Date'] = pd.to_datetime(weather_df['Date'])\n\nExploratory Data Analysis\n\n#Exploratory Data Analysis\n#Histograms and KDE (Kernel Density Estimation) plots for Temperature and Humidity.\nsns.histplot(weather_df['Temperature'], kde=True, color='blue', label='Temperature')\nsns.histplot(weather_df['Humidity'], kde=True, color='green', label='Humidity', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n# Pair Plot to visualize all variables together.\nsns.pairplot(weather_df, hue='Weather_Condition')\nplt.show()\n\n\n\n\nProbability Distributions\n\n#Normal Distribution Fit for Temperature.\nsns.histplot(weather_df['Temperature'], kde=False, color='blue', label='Temperature')\n\n# Fitting a normal distribution and plotting it\nmean, std = norm.fit(weather_df['Temperature'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p * max(weather_df['Temperature'].value_counts()), 'k', linewidth=2)\n\ntitle = \"Fit results: mean = %.2f,  std = %.2f\" % (mean, std)\nplt.title(title)\n\nplt.show()\n\n\n\n\n\n#Exponential Distribution Fit for Humidity.\nfrom scipy.stats import expon\n\n# Plotting histogram\nsns.histplot(weather_df['Humidity'], kde=False, color='green', label='Humidity')\n\n# Fitting an exponential distribution and plotting it\nparams = expon.fit(weather_df['Humidity'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = expon.pdf(x, *params)\nplt.plot(x, p * max(weather_df['Humidity'].value_counts()), 'k', linewidth=2)\n\nplt.show()\n\n\n\n\nTime Series Analysis\nTemperature Trend over Time.\n#Checking the temperature trned against time weather_df.set_index(‚ÄòDate‚Äô)[‚ÄòTemperature‚Äô].plot() plt.title(‚ÄúTemperature Trend Over Time‚Äù) plt.ylabel(‚ÄúTemperature‚Äù) plt.show()\n\n\n\n**Probability Theory in Action**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#Conditional Probability: Probability of High Humidity given Rainy Weather.\nhigh_humidity = weather_df['Humidity'] &gt; 80\nrainy_days = weather_df['Weather_Condition'] == 'Rainy'\nprob_high_humidity_given_rain = np.mean(high_humidity[rainy_days])\nprint(f\"Probability of High Humidity given Rainy Weather: {prob_high_humidity_given_rain}\")\n\nProbability of High Humidity given Rainy Weather: 0.27586206896551724\n\n:::\n\n#Joint Distribution: Temperature and Humidity.\nsns.jointplot(data=weather_df, x='Temperature', y='Humidity', kind='kde', color='red')\nplt.show()\n\n\n\n\nCorrelation Analysis\n\n#Correlation Heatmap\n# Selecting only numerical columns for correlation analysis\nnumerical_weather_df = weather_df.select_dtypes(include=[np.number])\n\n# Plotting the correlation heatmap\nsns.heatmap(numerical_weather_df.corr(), annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nLinear Regression for Temperature Prediction\n\n#Model Training and Evaluation.\n# Preparing data for linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Preparing data for linear regression\nX = weather_df[['Humidity']]\ny = weather_df['Temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Training the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Making predictions\ny_pred = model.predict(X_test)\n\n# Plotting actual vs predicted values\nplt.scatter(X_test, y_test, color='blue', label='Actual')\nplt.scatter(X_test, y_pred, color='red', label='Predicted')\nplt.xlabel('Humidity')\nplt.ylabel('Temperature')\nplt.title('Actual vs Predicted Temperature')\nplt.legend()\nplt.show()\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\n\n\nMean Squared Error: 24.010797366937965\n\n\nMarkov Chain for Weather Condition Transitions\n\n#Let's simulate a simple Markov chain to model the transitions between different weather conditions.\nimport pandas as pd\n\n# Calculating transition probabilities\nweather_conditions = weather_df['Weather_Condition'].unique()\ntransition_matrix = pd.DataFrame(index=weather_conditions, columns=weather_conditions).fillna(0)\n\nfor (prev, curr) in zip(weather_df['Weather_Condition'], weather_df['Weather_Condition'][1:]):\n    transition_matrix.at[prev, curr] += 1\n\n# Normalizing the rows to sum to 1\ntransition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0)\n\n# Display the transition matrix\nprint(transition_matrix)\n\n           Windy     Snowy    Cloudy     Foggy     Sunny     Rainy\nWindy   0.155116  0.165017  0.161716  0.194719  0.181518  0.141914\nSnowy   0.196667  0.166667  0.153333  0.166667  0.156667  0.160000\nCloudy  0.144543  0.182891  0.171091  0.165192  0.188791  0.147493\nFoggy   0.199346  0.124183  0.199346  0.147059  0.140523  0.189542\nSunny   0.167247  0.167247  0.222997  0.167247  0.128920  0.146341\nRainy   0.131488  0.179931  0.211073  0.166090  0.141869  0.169550\n\n\nThis code calculates the probabilities of transitioning from one weather condition to another. It‚Äôs a basic form of a Markov chain.\nMonte Carlo Simulation for Temperature Extremes\n\n#Use Monte Carlo simulation to estimate the probability of extreme temperature events.\nnp.random.seed(0)\nnum_simulations = 10000\nextreme_temp_count = 0\nextreme_temp_threshold = 30  # Define what you consider as extreme temperature\n\nfor _ in range(num_simulations):\n    simulated_temp = np.random.choice(weather_df['Temperature'])\n    if simulated_temp &gt; extreme_temp_threshold:\n        extreme_temp_count += 1\n\nprobability_of_extreme_temp = extreme_temp_count / num_simulations\nprint(f\"Probability of Extreme Temperature (&gt; {extreme_temp_threshold}¬∞C): {probability_of_extreme_temp}\")\n\nProbability of Extreme Temperature (&gt; 30¬∞C): 0.0226\n\n\nThis Monte Carlo simulation randomly samples temperatures from the dataset and calculates the probability of encountering temperatures above a certain threshold."
  },
  {
    "objectID": "posts/Random Processes/index.html#step-1.-write-out-the-linear-regression-equation",
    "href": "posts/Random Processes/index.html#step-1.-write-out-the-linear-regression-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 1. Write out the linear regression equation",
    "text": "Step 1. Write out the linear regression equation\n\\(\\Huge y=\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-2.-the-logistic-regression-equation-is-the-same-as-above-except-output-is-log-odds",
    "href": "posts/Random Processes/index.html#step-2.-the-logistic-regression-equation-is-the-same-as-above-except-output-is-log-odds",
    "title": "Probability Theory and Random Processes",
    "section": "Step 2. The logistic regression equation is the same as above except output is log odds",
    "text": "Step 2. The logistic regression equation is the same as above except output is log odds\n\\(\\Huge log(odds)=\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-3.-exponentiate-both-sides-of-the-logistic-regression-equation-to-get-odds",
    "href": "posts/Random Processes/index.html#step-3.-exponentiate-both-sides-of-the-logistic-regression-equation-to-get-odds",
    "title": "Probability Theory and Random Processes",
    "section": "Step 3. Exponentiate both sides of the logistic regression equation to get odds",
    "text": "Step 3. Exponentiate both sides of the logistic regression equation to get odds\n\\(\\Huge odds=e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-4.-write-out-the-probability-equation",
    "href": "posts/Random Processes/index.html#step-4.-write-out-the-probability-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 4. Write out the probability equation",
    "text": "Step 4. Write out the probability equation\n\\(\\Huge p=\\frac{odds}{1+odds}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-5.-plug-odds-from-step-3-into-the-probability-equation",
    "href": "posts/Random Processes/index.html#step-5.-plug-odds-from-step-3-into-the-probability-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 5. Plug odds (from step 3) into the probability equation",
    "text": "Step 5. Plug odds (from step 3) into the probability equation\n\\(\\Huge p=\\frac{e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}}{1+e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-6.-divide-the-numerator-and-denominator-by-the-odds-from-step-3",
    "href": "posts/Random Processes/index.html#step-6.-divide-the-numerator-and-denominator-by-the-odds-from-step-3",
    "title": "Probability Theory and Random Processes",
    "section": "Step 6. Divide the numerator and denominator by the odds (from step 3)",
    "text": "Step 6. Divide the numerator and denominator by the odds (from step 3)\n\\(\\Huge p=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1+...+\\beta_n x_n)}}\\)\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#conclusion",
    "href": "posts/Random Processes/index.html#conclusion",
    "title": "Probability Theory and Random Processes",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis shows more sophisticated ways of applying probability theory and random processes to the weather dataset, providing insights into weather patterns and temperature predictions.\nSummarized the findings from the above analysis. Discussed the relevance of these probabilistic models in understanding weather patterns."
  }
]